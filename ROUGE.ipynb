{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d1a8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "# import gensim.downloader as api\n",
    "import jieba\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe017d7c",
   "metadata": {
    "id": "jA_9c3SpLTUn"
   },
   "source": [
    "# Data Importing and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06baf15b",
   "metadata": {
    "id": "nU_gOCWtF8XY"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('corpus')\n",
    "files.remove('.DS_Store')\n",
    "files.remove('scores_ru-en.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for file_ in files:\n",
    "  name = file_.split('-')[0] + file_.split('-')[1]\n",
    "  vars()[name] = pd.read_csv(os.path.join('corpus', file_, 'scores.csv'))\n",
    "  vars()[name].drop(columns = ['source', 'annotators', 'avg-score'], inplace = True)\n",
    "  vars()[name]['z-score'] = scaler.fit_transform(vars()[name]['z-score'].values.reshape(-1,1)) #normalizing values betwewen 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96384c74",
   "metadata": {
    "id": "EcUEx6elHvwd"
   },
   "outputs": [],
   "source": [
    "english = csen.copy()\n",
    "for df in [deen, ruen, zhen]:\n",
    "  english = english.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d072be",
   "metadata": {
    "id": "8ziEKCv4LFwE"
   },
   "outputs": [],
   "source": [
    "finnish = enfi.copy()\n",
    "chinese = enzh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17acc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "english.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c17ec",
   "metadata": {},
   "source": [
    "# Cleaning the corpus (updated  cleaning function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd0c8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          lower = False,\n",
    "          keep_numbers = False,\n",
    "          keep_expression = False,\n",
    "          remove_char = False,\n",
    "          remove_stop = False,\n",
    "          remove_tag = False,\n",
    "          lemmatize = False,\n",
    "          stemmer = False,\n",
    "          english = True\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    if english:\n",
    "        lang = 'english'\n",
    "    else:\n",
    "        lang = 'finnish'\n",
    "    \n",
    "    stop = set(stopwords.words(lang))\n",
    "    stem = SnowballStemmer(lang)\n",
    "    \n",
    "    updates = []\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "            \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if not keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        #KEEP '?' and '!' AS TOKENS\n",
    "        if not keep_expression:\n",
    "            text = re.sub(\"[\\?|\\!]\", 'EXPRESSION', text)\n",
    "            \n",
    "        #REMOVE TAGS\n",
    "        if remove_tag:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "            \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if remove_char:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            if english:\n",
    "                lemma = WordNetLemmatizer()\n",
    "                text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(stem.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def clean_ch(text_list, keep_numbers=False, remove_punctuation=False, remove_stop = False, stopwords_set='merged'):\n",
    "    \"\"\"\n",
    "    Function that removes chinese stopwords\n",
    "    \n",
    "    :param stopwords_set: remove words of both sets (merged), just the 1st (fst) or just the second (snd) \n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    \n",
    "    zh_stopwords1 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords1.txt', 'r', encoding='utf-8').readlines()]\n",
    "    zh_stopwords2 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords2.txt', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    if stopwords_set == 'merged':\n",
    "        stop = list(set(zh_stopwords1 + zh_stopwords2))\n",
    "    elif stopwords_set == 'fst':\n",
    "        stop = zh_stopwords1\n",
    "    elif stopwords_set == 'snd':\n",
    "        stop = zh_stopwords2\n",
    "\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        # REMOVE PUNCTUATION\n",
    "        if remove_punctuation:\n",
    "            # https://stackoverflow.com/questions/36640587/how-to-remove-chinese-punctuation-in-python\n",
    "            punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "            text = re.sub(r\"[%s]+\" %punc, \"\", text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            pretext = text\n",
    "            text = ' '.join([word for word in jieba.cut(text) if word not in stop])\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef630f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En\n",
    "cleaning_dict = {'lower': False, 'keep_numbers': True, 'keep_expression': False, 'remove_char': True, 'remove_stop': True, 'remove_tag': False, 'lemmatize': False, 'stemmer': True}\n",
    "english_clean = pd.DataFrame()\n",
    "english_clean['z-score'] = english['z-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    english_clean[column] = clean(english[column], cleaning_dict)\n",
    "    \n",
    "# Fi\n",
    "finnish_clean = pd.DataFrame()\n",
    "for c in ['reference', 'translation']:\n",
    "    finnish_clean[c] = clean(finnish[c],\n",
    "                             lower = False,\n",
    "                             keep_numbers = False,\n",
    "                             keep_expression = True,\n",
    "                             remove_char = True,\n",
    "                             remove_stop = False,\n",
    "                             remove_tag = True,\n",
    "                             lemmatize = False,\n",
    "                             stemmer = True,\n",
    "                             english=False)\n",
    "finnish_clean['z-score'] = finnish['z-score']\n",
    "\n",
    "#Ch\n",
    "chinese_clean = pd.DataFrame()\n",
    "for c in ['reference', 'translation']:\n",
    "    chinese_clean[c] = clean_ch(chinese[c],\n",
    "                                keep_numbers = False,\n",
    "                                remove_punctuation = False,\n",
    "                                remove_stop = True,\n",
    "                                stopwords_set = 'snd')\n",
    "chinese_clean['z-score'] = chinese['z-score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb2aedf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSIS 科学家 AnthonyDelGenio 新闻稿 解释 “ GISS 模型 模拟 模...</td>\n",
       "      <td>戈达德 太空 研究所 科学家 安东尼 · 德尔 · 杰 尼奥 新闻 发布会 解释 “ 戈达德...</td>\n",
       "      <td>0.451621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中国 英国 女性 4x200mFreestreyWTE 最后 称为 “ 中国 14 岁 孩子...</td>\n",
       "      <td>参加 女子 4x200 米 自由泳 接力赛 决赛 中国 小将 艾衍 描述 “ 那名 14 岁...</td>\n",
       "      <td>0.246545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>来到 2012 队友 没有 好处</td>\n",
       "      <td>2012 队友 看好</td>\n",
       "      <td>0.198549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>去年 GoudianGroup 南非 南非 港口 出口 163 套 风力 发电 项目</td>\n",
       "      <td>去年 国电 集团 共计 163 套 风电 项目 陆续 连云港 港 出口 南非</td>\n",
       "      <td>0.216002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>指称 Kempinski 旅馆 \" 被捕 \" 满足 阿拉伯 客户 要求</td>\n",
       "      <td>有人 认为 凯宾斯基 酒店 简直 满足 阿拉伯 客户 要求 “ 卑躬屈膝 ”</td>\n",
       "      <td>0.391471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10216</th>\n",
       "      <td>大型 会议 皇家   Ascot   - 普通 星期五 晚上 20 000</td>\n",
       "      <td>一场 大规模 赛马会 英国皇家 爱斯科 赛马会   ( Royal   Ascot ) 有着...</td>\n",
       "      <td>0.688151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>这位 负责人 强调 钢铁 产品 国际贸易 本质 市场 行为 是从 进口国 要求 考虑 产品成...</td>\n",
       "      <td>这位 负责人 强调 钢铁 产品 国际贸易 本质 讲 市场 行为 源于 进口国 需求 消费者 ...</td>\n",
       "      <td>0.817705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>继续 浏览 最终   Bouverie   广场 购物中心 商店 里 尝试 四双 内衣</td>\n",
       "      <td>继续 浏览 货架 最终 位于   Folkestone     Bouverie   Pla...</td>\n",
       "      <td>0.711189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>带 女儿 参加 综艺节目 “ 爸爸 ” 成为 焦点</td>\n",
       "      <td>2004 奥运会 单人 十米 台 比赛 输给 队友 胡佳 田亮 竞技状态 出现 下滑 爆出 ...</td>\n",
       "      <td>0.222476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10220</th>\n",
       "      <td>行业 数据 显示 2015 煤炭 生产 总量 接近 60 亿吨 全国 煤炭 生产 产能 过剩...</td>\n",
       "      <td>行业 数据 显示 2015 煤炭 产能 总 规模 接近 60 亿吨 全国 煤炭 产能 过剩 ...</td>\n",
       "      <td>0.903920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10221 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reference  \\\n",
       "0      GSIS 科学家 AnthonyDelGenio 新闻稿 解释 “ GISS 模型 模拟 模...   \n",
       "1      中国 英国 女性 4x200mFreestreyWTE 最后 称为 “ 中国 14 岁 孩子...   \n",
       "2                                       来到 2012 队友 没有 好处   \n",
       "3             去年 GoudianGroup 南非 南非 港口 出口 163 套 风力 发电 项目   \n",
       "4                    指称 Kempinski 旅馆 \" 被捕 \" 满足 阿拉伯 客户 要求   \n",
       "...                                                  ...   \n",
       "10216              大型 会议 皇家   Ascot   - 普通 星期五 晚上 20 000   \n",
       "10217  这位 负责人 强调 钢铁 产品 国际贸易 本质 市场 行为 是从 进口国 要求 考虑 产品成...   \n",
       "10218        继续 浏览 最终   Bouverie   广场 购物中心 商店 里 尝试 四双 内衣   \n",
       "10219                          带 女儿 参加 综艺节目 “ 爸爸 ” 成为 焦点   \n",
       "10220  行业 数据 显示 2015 煤炭 生产 总量 接近 60 亿吨 全国 煤炭 生产 产能 过剩...   \n",
       "\n",
       "                                             translation   z-score  \n",
       "0      戈达德 太空 研究所 科学家 安东尼 · 德尔 · 杰 尼奥 新闻 发布会 解释 “ 戈达德...  0.451621  \n",
       "1      参加 女子 4x200 米 自由泳 接力赛 决赛 中国 小将 艾衍 描述 “ 那名 14 岁...  0.246545  \n",
       "2                                             2012 队友 看好  0.198549  \n",
       "3                 去年 国电 集团 共计 163 套 风电 项目 陆续 连云港 港 出口 南非  0.216002  \n",
       "4                 有人 认为 凯宾斯基 酒店 简直 满足 阿拉伯 客户 要求 “ 卑躬屈膝 ”  0.391471  \n",
       "...                                                  ...       ...  \n",
       "10216  一场 大规模 赛马会 英国皇家 爱斯科 赛马会   ( Royal   Ascot ) 有着...  0.688151  \n",
       "10217  这位 负责人 强调 钢铁 产品 国际贸易 本质 讲 市场 行为 源于 进口国 需求 消费者 ...  0.817705  \n",
       "10218  继续 浏览 货架 最终 位于   Folkestone     Bouverie   Pla...  0.711189  \n",
       "10219  2004 奥运会 单人 十米 台 比赛 输给 队友 胡佳 田亮 竞技状态 出现 下滑 爆出 ...  0.222476  \n",
       "10220  行业 数据 显示 2015 煤炭 产能 总 规模 接近 60 亿吨 全国 煤炭 产能 过剩 ...  0.903920  \n",
       "\n",
       "[10221 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b1f83",
   "metadata": {
    "id": "NjBupUyDLNRa"
   },
   "source": [
    "# Train, Dev & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8d0830c",
   "metadata": {
    "id": "qZKkQkBNLSYT"
   },
   "outputs": [],
   "source": [
    "# English\n",
    "en_train, en_dev = train_test_split(english, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev, en_test = train_test_split(en_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "en_train_clean, en_dev_clean = train_test_split(english_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev_clean, en_test_clean = train_test_split(en_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "\n",
    "# Finnish\n",
    "fin_train, fin_dev = train_test_split(finnish, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "fin_dev, fin_test = train_test_split(fin_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "fin_train_clean, fin_dev_clean = train_test_split(finnish_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "fin_dev_clean, fin_test_clean = train_test_split(fin_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "#Chinese\n",
    "ch_train, ch_dev = train_test_split(chinese, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "ch_dev, ch_test = train_test_split(ch_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "ch_train_clean, ch_dev_clean = train_test_split(chinese_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "ch_dev_clean, ch_test_clean = train_test_split(ch_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3d238",
   "metadata": {},
   "source": [
    "# Not cleaned corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835f718",
   "metadata": {},
   "source": [
    "## Encoding (Word2Vec + Word Mover Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f310281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9987cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['en_train', 'en_dev', 'en_test']\n",
    "for j,df in enumerate([en_train, en_dev, en_test]):\n",
    "    name = 'distances_' + names[j]\n",
    "    vars()[name] = []\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    for i in tqdm(range(len(df))):\n",
    "                  \n",
    "        vars()[name].append(model.wmdistance(df['reference'][i], df['translation'][i]))\n",
    "    \n",
    "    name2 = 'score_' + names[j]\n",
    "    vars()[name2] = np.array(df['z-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fe9b5",
   "metadata": {},
   "source": [
    "## Calculating correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6734f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_train_true, y_train_pred, y_dev_true, y_dev_pred, y_test_true, y_test_pred, return_corr = False):\n",
    "    \n",
    "    cleaned_corr_train, cleaned_corr_train_pvalue = pearsonr(y_train_true, y_train_pred)\n",
    "    cleaned_corr_ktau_train, cleaned_corr_ktau_train_pvalue = kendalltau(y_train_true, y_train_pred)\n",
    "    \n",
    "    cleaned_corr_dev, cleaned_corr_dev_pvalue = pearsonr(y_dev_true, y_dev_pred)\n",
    "    cleaned_corr_ktau_dev, cleaned_corr_ktau_dev_pvalue = kendalltau(y_dev_true, y_dev_pred)\n",
    "\n",
    "\n",
    "    cleaned_corr_ktau_test, cleaned_corr_ktau_test_pvalue = kendalltau(y_test_true, y_test_pred)\n",
    "    cleaned_corr_test, cleaned_corr_test_pvalue = pearsonr(y_test_true, y_test_pred)\n",
    "        \n",
    "    print(f'Pearson correlation between cosine similarity and score on train set: {cleaned_corr_train} (p-value < 0.001: {cleaned_corr_train_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_train} (p-value < 0.001: {cleaned_corr_ktau_train_pvalue < 0.001})')\n",
    "    print(f'Pearson correlation between cosine similarity and score on development set: {cleaned_corr_dev} (p-value < 0.001: {cleaned_corr_dev_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_dev} (p-value < 0.001: {cleaned_corr_ktau_dev_pvalue < 0.001})')\n",
    "    print(f'Pearson correlation between cosine similarity and score on test set: {cleaned_corr_test} (p-value < 0.001: {cleaned_corr_test_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_test} (p-value < 0.001: {cleaned_corr_ktau_test_pvalue < 0.001})')\n",
    "    \n",
    "    if return_corr:\n",
    "        return cleaned_corr_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31ea45",
   "metadata": {},
   "source": [
    "How to treat np.inf???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c612bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.inf in distances_en_train:\n",
    "    distances_en_train[distances_en_train.index(np.inf)] = 10\n",
    "\n",
    "while np.inf in distances_en_dev:\n",
    "    distances_en_dev[distances_en_dev.index(np.inf)] = 10\n",
    "    \n",
    "while np.inf in distances_en_test:\n",
    "    distances_en_test[distances_en_test.index(np.inf)] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4812a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr(score_en_train, distances_en_train, score_en_dev, distances_en_dev, score_en_test, distances_en_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e47e",
   "metadata": {},
   "source": [
    "# Cleaned corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e8810",
   "metadata": {},
   "source": [
    "## Encoding (Word2Vec + Word Mover Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856aed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = api.load('word2vec-google-news-300')\n",
    "names = ['en_train_cleaned', 'en_dev_cleaned', 'en_test_cleaned']\n",
    "for j,df in enumerate([en_train_cleaned, en_dev_cleaned, en_test_cleaned]):\n",
    "    name = 'distances_' + names[j]\n",
    "    vars()[name] = []\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    for i in tqdm(range(len(df))):\n",
    "                  \n",
    "        vars()[name].append(model.wmdistance(df['reference'][i], df['translation'][i]))\n",
    "    \n",
    "    name2 = 'score_' + names[j]\n",
    "    vars()[name2] = np.array(df['z-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df31e74",
   "metadata": {},
   "source": [
    "## Calculating correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.inf in distances_en_train_cleaned:\n",
    "    distances_en_train_cleaned[distances_en_train_cleaned.index(np.inf)] = 10\n",
    "\n",
    "while np.inf in distances_en_dev_cleaned:\n",
    "    distances_en_dev_cleaned[distances_en_dev_cleaned.index(np.inf)] = 10\n",
    "    \n",
    "while np.inf in distances_en_test_cleaned:\n",
    "    distances_en_test_cleaned[distances_en_test_cleaned.index(np.inf)] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr(score_en_train_cleaned, distances_en_train_cleaned, score_en_dev_cleaned, distances_en_dev_cleaned, score_en_test_cleaned, distances_en_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a573d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ch_dev_clean.copy()\n",
    "train=train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05560ef",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e5e9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['rouge-1','rouge-2','rouge-l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97a1e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(df,metric):\n",
    "    rouge = Rouge()\n",
    "    references =df['reference'].to_list()\n",
    "    translation =df['translation'].to_list()\n",
    "    scores=rouge.get_scores(translation, references)\n",
    "    rougedf = pd.DataFrame()\n",
    "\n",
    "    for score in scores:\n",
    "        new_row = score[metric]\n",
    "        rougedf = rougedf.append(new_row, ignore_index=True)\n",
    "    rougedf.rename(columns={'f':'F1','p':'Precision','r':'Recall'},inplace=True)\n",
    "    rougedf['Z-score']=train.iloc[:,-1]\n",
    "    \n",
    "    \n",
    "        \n",
    "    return(rougedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ac0d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation scores for rouge-1 metric \n",
      "\n",
      "F1\n",
      "Pearson correlation between RougeScore and score on development set: 0.4560311953072406 (p-value < 0.001: True); and Kendall Tau correlation: 0.31476099352769943 (p-value < 0.001: True)\n",
      "Precision\n",
      "Pearson correlation between RougeScore and score on development set: 0.45844589837035377 (p-value < 0.001: True); and Kendall Tau correlation: 0.32373343209892613 (p-value < 0.001: True)\n",
      "Recall\n",
      "Pearson correlation between RougeScore and score on development set: 0.4200236411485582 (p-value < 0.001: True); and Kendall Tau correlation: 0.28901528534576454 (p-value < 0.001: True)\n",
      "\n",
      "\n",
      "Correlation scores for rouge-2 metric \n",
      "\n",
      "F1\n",
      "Pearson correlation between RougeScore and score on development set: 0.38529887403362006 (p-value < 0.001: True); and Kendall Tau correlation: 0.2800140323240911 (p-value < 0.001: True)\n",
      "Precision\n",
      "Pearson correlation between RougeScore and score on development set: 0.3851102389522171 (p-value < 0.001: True); and Kendall Tau correlation: 0.28579130953578585 (p-value < 0.001: True)\n",
      "Recall\n",
      "Pearson correlation between RougeScore and score on development set: 0.37623157265512863 (p-value < 0.001: True); and Kendall Tau correlation: 0.2737728833662506 (p-value < 0.001: True)\n",
      "\n",
      "\n",
      "Correlation scores for rouge-l metric \n",
      "\n",
      "F1\n",
      "Pearson correlation between RougeScore and score on development set: 0.4766929600222844 (p-value < 0.001: True); and Kendall Tau correlation: 0.3322986530067027 (p-value < 0.001: True)\n",
      "Precision\n",
      "Pearson correlation between RougeScore and score on development set: 0.4818698623528249 (p-value < 0.001: True); and Kendall Tau correlation: 0.3414012576652352 (p-value < 0.001: True)\n",
      "Recall\n",
      "Pearson correlation between RougeScore and score on development set: 0.44431666485674853 (p-value < 0.001: True); and Kendall Tau correlation: 0.31062129007095624 (p-value < 0.001: True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    rougedf=rouge_score(train,metric)\n",
    "    rougedf.corr(method='pearson')\n",
    "    score = rougedf['Z-score']\n",
    "    \n",
    "    rougescore_corr, rougescore_corr_pvalue = pearsonr(score,rougedf['F1'])\n",
    "    rougescore_corr_ktau, rougescore_corr_ktau_pvalue = kendalltau(score, rougedf['F1'])\n",
    "    print('Correlation scores for',metric,'metric \\n')\n",
    "    print('F1')\n",
    "    print(f'Pearson correlation between RougeScore and score on development set: {rougescore_corr} (p-value < 0.001: {rougescore_corr_pvalue < 0.001}); and Kendall Tau correlation: {rougescore_corr_ktau} (p-value < 0.001: {rougescore_corr_ktau_pvalue < 0.001})')\n",
    "  \n",
    "    rougescore_corr, rougescore_corr_pvalue = pearsonr(score,rougedf['Precision'])\n",
    "    rougescore_corr_ktau, rougescore_corr_ktau_pvalue = kendalltau(score, rougedf['Precision'])\n",
    "    \n",
    "    print('Precision')\n",
    "    print(f'Pearson correlation between RougeScore and score on development set: {rougescore_corr} (p-value < 0.001: {rougescore_corr_pvalue < 0.001}); and Kendall Tau correlation: {rougescore_corr_ktau} (p-value < 0.001: {rougescore_corr_ktau_pvalue < 0.001})')\n",
    "    \n",
    "    \n",
    "    rougescore_corr, rougescore_corr_pvalue = pearsonr(score,rougedf['Recall'])\n",
    "    rougescore_corr_ktau, rougescore_corr_ktau_pvalue = kendalltau(score, rougedf['Recall'])\n",
    "    print('Recall')\n",
    "    print(f'Pearson correlation between RougeScore and score on development set: {rougescore_corr} (p-value < 0.001: {rougescore_corr_pvalue < 0.001}); and Kendall Tau correlation: {rougescore_corr_ktau} (p-value < 0.001: {rougescore_corr_ktau_pvalue < 0.001})')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f59f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "rougedf=rouge_score(train,'rouge-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0b7d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rouge = rouge_score(train[['reference', 'translation']],'rouge-l')['F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d4b259d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Z-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.752975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.207275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.722492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.837772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.487816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.595979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.393515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.648436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.754224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1022 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            F1  Precision    Recall   Z-score\n",
       "0     0.444444   0.461538  0.428571  0.752975\n",
       "1     0.473684   0.409091  0.562500  0.207275\n",
       "2     0.590909   0.565217  0.619048  0.722492\n",
       "3     0.375000   0.428571  0.333333  0.837772\n",
       "4     0.315789   0.285714  0.352941  0.487816\n",
       "...        ...        ...       ...       ...\n",
       "1017  0.333333   0.250000  0.500000  0.595979\n",
       "1018  0.454545   0.454545  0.454545  0.393515\n",
       "1019  0.260870   0.272727  0.250000  0.648436\n",
       "1020  0.551724   0.571429  0.533333  0.754224\n",
       "1021  0.666667   0.500000  1.000000  0.805958\n",
       "\n",
       "[1022 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab085048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_train_true, y_train_pred, return_corr = False):\n",
    "    \n",
    "    cleaned_corr_train, cleaned_corr_train_pvalue = pearsonr(y_train_true, y_train_pred)\n",
    "    cleaned_corr_ktau_train, cleaned_corr_ktau_train_pvalue = kendalltau(y_train_true, y_train_pred)\n",
    "        \n",
    "    print(f'Train - Pearson: {cleaned_corr_train}; Kendall Tau: {cleaned_corr_ktau_train}')\n",
    "    \n",
    "    if return_corr:\n",
    "        return [cleaned_corr_train, cleaned_corr_ktau_train, \n",
    "                cleaned_corr_dev, cleaned_corr_ktau_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rougescore_corr, rougescore_corr_pvalue = pearsonr(score,rougedf['F1'])\n",
    "rougescore_corr_ktau, rougescore_corr_ktau_pvalue = kendalltau(score, rougedf['F1'])\n",
    "print('Correlation scores for',metric,'metric \\n')\n",
    "print('F1')\n",
    "print(f'Pearson correlation between RougeScore and score on development set: {rougescore_corr} (p-value < 0.001: {rougescore_corr_pvalue < 0.001}); and Kendall Tau correlation: {rougescore_corr_ktau} (p-value < 0.001: {rougescore_corr_ktau_pvalue < 0.001})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f84a7187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Pearson: 0.4766929600222844; Kendall Tau: 0.3322986530067027\n"
     ]
    }
   ],
   "source": [
    "corr(f1_rouge, rougedf['Z-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3265516",
   "metadata": {},
   "source": [
    "# Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e139d9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The future and the destinies of the citizens o...</td>\n",
       "      <td>世界上每个国家公民的未来和命运日益联系在一起。</td>\n",
       "      <td>世界各国人民前途命运越来越紧密地联系在一起。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After all that hard work, the finished result ...</td>\n",
       "      <td>经过那么多的努力，最终的结果现在已经可以揭晓了。</td>\n",
       "      <td>经过这么艰辛的工作，最终的结果现在才得以公布。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Author: researcher of Suning Institute of Fina...</td>\n",
       "      <td>作者：苏宁金融研究所研究员，财经专栏作家，财经评论员。</td>\n",
       "      <td>作者：苏宁金融研究院特约研究员，财经专栏作家，财经评论员。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The Great Wall” tells the story of a Chinese ...</td>\n",
       "      <td>《长城》讲述了古代一支中国精锐部队在世界著名的中国长城上与怪物桃蒂英勇作战的故事。</td>\n",
       "      <td>《长城》讲述了在古代，一支中国精英部队为保卫人类，在举世闻名的长城上与怪兽饕餮进行生死决战的故事。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our comrades from the Political Bureau should ...</td>\n",
       "      <td>政治局同志要学习历史，讲道理，不能混淆公、私利益，叫白黑，模糊义与利的界限，处理基于裙带关系...</td>\n",
       "      <td>中央政治局的同志都应该明史知理，不能颠倒了公私、混淆了是非、模糊了义利、放纵了亲情，要带头树...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  The future and the destinies of the citizens o...   \n",
       "1  After all that hard work, the finished result ...   \n",
       "2  Author: researcher of Suning Institute of Fina...   \n",
       "3  “The Great Wall” tells the story of a Chinese ...   \n",
       "4  Our comrades from the Political Bureau should ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0                            世界上每个国家公民的未来和命运日益联系在一起。   \n",
       "1                           经过那么多的努力，最终的结果现在已经可以揭晓了。   \n",
       "2                        作者：苏宁金融研究所研究员，财经专栏作家，财经评论员。   \n",
       "3          《长城》讲述了古代一支中国精锐部队在世界著名的中国长城上与怪物桃蒂英勇作战的故事。   \n",
       "4  政治局同志要学习历史，讲道理，不能混淆公、私利益，叫白黑，模糊义与利的界限，处理基于裙带关系...   \n",
       "\n",
       "                                         translation  \n",
       "0                             世界各国人民前途命运越来越紧密地联系在一起。  \n",
       "1                            经过这么艰辛的工作，最终的结果现在才得以公布。  \n",
       "2                      作者：苏宁金融研究院特约研究员，财经专栏作家，财经评论员。  \n",
       "3  《长城》讲述了在古代，一支中国精英部队为保卫人类，在举世闻名的长城上与怪兽饕餮进行生死决战的故事。  \n",
       "4  中央政治局的同志都应该明史知理，不能颠倒了公私、混淆了是非、模糊了义利、放纵了亲情，要带头树...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCh = pd.read_csv('testset/en-zh/scores.csv')\n",
    "testCh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e991954b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>世界 每个 国家 公民 未来 命运 日益 联系 一起</td>\n",
       "      <td>世界 各国 人民 前途 命运 越来越 紧密 联系 一起</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>努力 最终 现在 已经 揭晓</td>\n",
       "      <td>艰辛 工作 最终 现在 得以 公布</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者 苏宁 金融 研究所 研究员 财经 专栏作家 财经 评论员</td>\n",
       "      <td>作者 苏宁 金融 研究院 特约 研究员 财经 专栏作家 财经 评论员</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>长城 讲述 古代 一支 中国 精锐部队 世界 著名 中国 长城 怪物 桃蒂 英勇作战 故事</td>\n",
       "      <td>长城 讲述 古代 一支 中国 精英 部队 保卫 人类 举世闻名 长城 怪兽 饕餮 进行 生死...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>政治局 同志 学习 历史 讲道理 不能 混淆 公 私 利益 叫白 黑 模糊 义与利 界限 处...</td>\n",
       "      <td>中央政治局 同志 应该 明史 知理 不能 颠倒 公私 混淆 是非 模糊 义利 放纵 亲情 要...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>这为 年复一年 保持良好 丰收 打下 坚实 基础 ,   不断 增加 农民收入 ,   农村...</td>\n",
       "      <td>实现 农业 连年丰收 农民 持续 增收 农村 经济社会 健康 发展 提供 强有力 基础 支撑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22124</th>\n",
       "      <td>CCTV - 2   美国 国家 经济 战略 研究院 编写 一堆 干货 ,   这份 中国 ...</td>\n",
       "      <td>央视 财经频道 联合 中国社科院 财经 战略 研究院 梳理 一份 干货 这份 沉甸甸 中国 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>肯塔基州 ,   贝文   ( R )   得到 美国 公民自由 联盟 使用   Faceb...</td>\n",
       "      <td>肯塔基州 州长 马特 · 贝文   ( R )   脸书 推特 使用 美国 公民自由 联盟 光顾</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22126</th>\n",
       "      <td>新 锦屏 总书记 党中央 村民 关注 ,</td>\n",
       "      <td>老百姓 传 达到 把习 总书记 党中央 关怀 关心 传 达到 觉得</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>第一颗 人造卫星 中国 太空科学 卫星 系列 世界 第一个 暗物质 粒子 探测 卫星 ,  ...</td>\n",
       "      <td>我国 空间科学 卫星 系列 首发 星 世界 首颗 暗物质 粒子 探测 卫星 悟空</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22128 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reference  \\\n",
       "0                             世界 每个 国家 公民 未来 命运 日益 联系 一起   \n",
       "1                                         努力 最终 现在 已经 揭晓   \n",
       "2                        作者 苏宁 金融 研究所 研究员 财经 专栏作家 财经 评论员   \n",
       "3          长城 讲述 古代 一支 中国 精锐部队 世界 著名 中国 长城 怪物 桃蒂 英勇作战 故事   \n",
       "4      政治局 同志 学习 历史 讲道理 不能 混淆 公 私 利益 叫白 黑 模糊 义与利 界限 处...   \n",
       "...                                                  ...   \n",
       "22123  这为 年复一年 保持良好 丰收 打下 坚实 基础 ,   不断 增加 农民收入 ,   农村...   \n",
       "22124  CCTV - 2   美国 国家 经济 战略 研究院 编写 一堆 干货 ,   这份 中国 ...   \n",
       "22125  肯塔基州 ,   贝文   ( R )   得到 美国 公民自由 联盟 使用   Faceb...   \n",
       "22126                             新 锦屏 总书记 党中央 村民 关注 ,     \n",
       "22127  第一颗 人造卫星 中国 太空科学 卫星 系列 世界 第一个 暗物质 粒子 探测 卫星 ,  ...   \n",
       "\n",
       "                                             translation  \n",
       "0                            世界 各国 人民 前途 命运 越来越 紧密 联系 一起  \n",
       "1                                      艰辛 工作 最终 现在 得以 公布  \n",
       "2                     作者 苏宁 金融 研究院 特约 研究员 财经 专栏作家 财经 评论员  \n",
       "3      长城 讲述 古代 一支 中国 精英 部队 保卫 人类 举世闻名 长城 怪兽 饕餮 进行 生死...  \n",
       "4      中央政治局 同志 应该 明史 知理 不能 颠倒 公私 混淆 是非 模糊 义利 放纵 亲情 要...  \n",
       "...                                                  ...  \n",
       "22123     实现 农业 连年丰收 农民 持续 增收 农村 经济社会 健康 发展 提供 强有力 基础 支撑  \n",
       "22124  央视 财经频道 联合 中国社科院 财经 战略 研究院 梳理 一份 干货 这份 沉甸甸 中国 ...  \n",
       "22125   肯塔基州 州长 马特 · 贝文   ( R )   脸书 推特 使用 美国 公民自由 联盟 光顾  \n",
       "22126                  老百姓 传 达到 把习 总书记 党中央 关怀 关心 传 达到 觉得  \n",
       "22127           我国 空间科学 卫星 系列 首发 星 世界 首颗 暗物质 粒子 探测 卫星 悟空  \n",
       "\n",
       "[22128 rows x 2 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ch\n",
    "test_ch = pd.DataFrame()\n",
    "for c in ['reference', 'translation']:\n",
    "    test_ch[c] = clean_ch(testCh[c],\n",
    "                                keep_numbers = False,\n",
    "                                remove_punctuation = False,\n",
    "                                remove_stop = True,\n",
    "                                stopwords_set = 'snd')\n",
    "test_ch = test_ch.replace(r'^\\s*$', 'stop', regex=True)\n",
    "test_ch.loc[18438, 'reference'] = 'stop'\n",
    "test_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5b5c7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ch['metric'] = rouge_score(test_ch,'rouge-l')['F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c6604a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ch.to_csv('test_scores/en-zh_metric_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "112ba676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(test_ch['reference'].tolist())):\n",
    "#     if len(str(test_ch.loc[i, 'reference']))<2:\n",
    "#         print('--->', i)\n",
    "#         print(test_ch.loc[i])\n",
    "#         print(Rouge().get_scores(test_ch.loc[i, 'reference'], test_ch.loc[i, 'translation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e07b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
