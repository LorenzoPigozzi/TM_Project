{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1705b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utente\\anaconda3\\envs\\text\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873d7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce514b72",
   "metadata": {
    "id": "jA_9c3SpLTUn"
   },
   "source": [
    "# Data Importing and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb58d83f",
   "metadata": {
    "id": "nU_gOCWtF8XY"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('corpus')\n",
    "files.remove('.DS_Store')\n",
    "files.remove('scores_ru-en.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for file_ in files:\n",
    "  name = file_.split('-')[0] + file_.split('-')[1]\n",
    "  vars()[name] = pd.read_csv(os.path.join('corpus', file_, 'scores.csv'))\n",
    "  vars()[name].drop(columns = ['source', 'annotators', 'z-score'], inplace = True)\n",
    "  vars()[name]['avg-score'] = scaler.fit_transform(vars()[name]['avg-score'].values.reshape(-1,1)) #normalizing values betwewen 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1ef1c2",
   "metadata": {
    "id": "EcUEx6elHvwd"
   },
   "outputs": [],
   "source": [
    "english = csen.copy()\n",
    "for df in [deen, ruen, zhen]:\n",
    "  english = english.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074be1cf",
   "metadata": {
    "id": "8ziEKCv4LFwE"
   },
   "outputs": [],
   "source": [
    "finnish = enfi.copy()\n",
    "chinese = enzh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ddc2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "english.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c2d59",
   "metadata": {},
   "source": [
    "# Cleaning the corpus (updated  cleaning function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50996387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          command_dict = None,\n",
    "#           lower = False,\n",
    "#           keep_numbers = False,\n",
    "#           keep_expression = False,\n",
    "#           remove_char = False,\n",
    "#           remove_stop = False,\n",
    "#           remove_tag = False,\n",
    "#           lemmatize = False,\n",
    "#           stemmer = False,\n",
    "          english = True\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    \n",
    "    if english:\n",
    "        lang = 'english'\n",
    "    else:\n",
    "        lang = 'finnish'\n",
    "    \n",
    "    stop = set(stopwords.words(lang))\n",
    "    stem = SnowballStemmer(lang)\n",
    "    \n",
    "    updates = []\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if command_dict['lower']:\n",
    "            text = text.lower()\n",
    "            \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if command_dict['keep_numbers']:\n",
    "            text = re.sub(\"[\\d+]\", 'NUMBER', text)\n",
    "        \n",
    "        #KEEP '?' and '!' AS TOKENS\n",
    "        if command_dict['keep_expression']:\n",
    "            text = re.sub(\"[\\?|\\!]\", 'EXPRESSION', text)\n",
    "            \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if command_dict['remove_char']:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "            \n",
    "        #REMOVE TAGS\n",
    "        if command_dict['remove_tag']:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if command_dict['remove_stop']:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if command_dict['lemmatize']:\n",
    "            if english:\n",
    "                lemma = WordNetLemmatizer()\n",
    "                text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "#             else:\n",
    "#                 lemma = libvoikko.Voikko(u\"fi\")\n",
    "#                 text = \" \".join(lemma.analyze(word)[0]['BASEFORM'] for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if command_dict['stemmer']:\n",
    "            text = \" \".join(stem.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def clean_zh_stopwords(text_list, stopwords_set='merged'):\n",
    "    \"\"\"\n",
    "    Function that removes chinese stopwords\n",
    "    \n",
    "    :param stopwords_set: remove words of both sets (merged), just the 1st (fst) or just the second (snd) \n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    \n",
    "    zh_stopwords1 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords1.txt', 'r', encoding='utf-8').readlines()]\n",
    "    zh_stopwords2 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords2.txt', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    if stopwords_set == 'merged':\n",
    "        stop = list(set(zh_stopwords1 + zh_stopwords2))\n",
    "    elif stopwords_set == 'fst':\n",
    "        stop = zh_stopwords1\n",
    "    elif stopwords_set == 'snd':\n",
    "        stop = zh_stopwords2\n",
    "        \n",
    "\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        text = text_list[j]\n",
    "        text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "        \n",
    "    \n",
    "def update_df(dataframe, list_updated):\n",
    "    dataframe.update(pd.DataFrame({\"Text\": list_updated}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c8a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2b498a60214f59bb9c0d06da921644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ce7ff8c05f48918cd40910d568373a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaning_dict = {'lower': False, 'keep_numbers': True, 'keep_expression': False, 'remove_char': True, 'remove_stop': True, 'remove_tag': False, 'lemmatize': False, 'stemmer': True}\n",
    "english_cleaned = pd.DataFrame()\n",
    "english_cleaned['avg-score'] = english['avg-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    english_cleaned[column] = clean(english[column], cleaning_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e63d2",
   "metadata": {
    "id": "NjBupUyDLNRa"
   },
   "source": [
    "# Train, Dev & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5d3aa6",
   "metadata": {
    "id": "qZKkQkBNLSYT"
   },
   "outputs": [],
   "source": [
    "en_train_cleaned, en_dev_cleaned = train_test_split(english_cleaned, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev_cleaned, en_test_cleaned = train_test_split(en_dev_cleaned, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "##NEED TO IMPLEMENT\n",
    "# fin_train, fin_dev = train_test_split(finnish, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "# fin_dev, fin_test = train_test_split(fin_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "# ch_train, ch_dev = train_test_split(chinese, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "# ch_dev, ch_test = train_test_split(ch_dev, shuffle = True, test_size = 0.5, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f89e9",
   "metadata": {},
   "source": [
    "# Encoding with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8cae469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04d0fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098cd2c324684bcc9c8043f32a6693a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall encoding process: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13abf5f4089438b85571a441a1c6a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering en_train_cleaned reference:   0%|          | 0/62150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cdc1cbcfef404ea209986c0c79f7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding en_train_cleaned reference:   0%|          | 0/62150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc921b67e961452ead61503316342423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering en_train_cleaned translation:   0%|          | 0/62150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a6b517878247a889d48526217f5138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding en_train_cleaned translation:   0%|          | 0/62150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5310178db4d43a1b0d41b64825877ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering en_dev_cleaned reference:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15952ff5c47944bc99aff0c773710c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding en_dev_cleaned reference:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a363c80ecf24dc88a67de6be15f2d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering en_dev_cleaned translation:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74616d0b008b404db3d747f7d6bc42eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding en_dev_cleaned translation:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0ab13ac3684f8eb398df61456f51b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering en_test_cleaned reference:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6b20deaa26453bb062f92e62d95067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding en_test_cleaned reference:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da4a4659e2c40a9bb6ddf49d21a021a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering en_test_cleaned translation:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d7f6e700bb42d2abe5a1dbf0963c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding en_test_cleaned translation:   0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = ['en_train_cleaned', 'en_dev_cleaned', 'en_test_cleaned']\n",
    "for j,df in tqdm(enumerate([en_train_cleaned, en_dev_cleaned, en_test_cleaned]), desc = 'Overall encoding process'):\n",
    "    name = 'encoded_' + names[j]\n",
    "    vars()[name] = pd.DataFrame()\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    for column in ['reference', 'translation']:\n",
    "        \n",
    "        ready_for_encoding = [sent.split(' ') for sent in df[column]]\n",
    "        filtered_doc = []\n",
    "        for doc in tqdm(ready_for_encoding, desc = f'Filtering {names[j]} {column}'):\n",
    "            words = filter(lambda x: x in model.index_to_key, doc)\n",
    "            filtered_doc.append(words)\n",
    "        vars()[name][column] = list(map(lambda x: [model.get_vector(word) for word in x], tqdm(filtered_doc, desc = f'Encoding {names[j]} {column}')))\n",
    "        \n",
    "    vars()[name].to_csv(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58d525",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e5180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b25b4443",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cea58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
