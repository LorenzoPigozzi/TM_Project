{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1705b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "# import libvoikko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873d7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdcce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install libvoikko\n",
    "# !pip install voikko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce514b72",
   "metadata": {
    "id": "jA_9c3SpLTUn"
   },
   "source": [
    "# Data Importing and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb58d83f",
   "metadata": {
    "id": "nU_gOCWtF8XY"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('corpus')\n",
    "files.remove('.DS_Store')\n",
    "files.remove('scores_ru-en.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for file_ in files:\n",
    "  name = file_.split('-')[0] + file_.split('-')[1]\n",
    "  vars()[name] = pd.read_csv(os.path.join('corpus', file_, 'scores.csv'))\n",
    "  vars()[name].drop(columns = ['source', 'annotators', 'z-score'], inplace = True)\n",
    "  vars()[name]['avg-score'] = scaler.fit_transform(vars()[name]['avg-score'].values.reshape(-1,1)) #normalizing values betwewen 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1ef1c2",
   "metadata": {
    "id": "EcUEx6elHvwd"
   },
   "outputs": [],
   "source": [
    "english = csen.copy()\n",
    "for df in [deen, ruen, zhen]:\n",
    "  english = english.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074be1cf",
   "metadata": {
    "id": "8ziEKCv4LFwE"
   },
   "outputs": [],
   "source": [
    "finnish = enfi.copy()\n",
    "chinese = enzh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddc2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "english.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c2d59",
   "metadata": {},
   "source": [
    "# Cleaning the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50996387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          lower = False,\n",
    "          keep_numbers = False,\n",
    "          keep_expression = False,\n",
    "          remove_char = False,\n",
    "          remove_stop = False,\n",
    "          remove_tag = False,\n",
    "          lemmatize = False,\n",
    "          stemmer = False,\n",
    "          english = True\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    if english:\n",
    "        lang = 'english'\n",
    "    else:\n",
    "        lang = 'finnish'\n",
    "    \n",
    "    stop = set(stopwords.words(lang))\n",
    "    stem = SnowballStemmer(lang)\n",
    "    \n",
    "    updates = []\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "            \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'NUMBER', text)\n",
    "        \n",
    "        #KEEP '?' and '!' AS TOKENS\n",
    "        if keep_expression:\n",
    "            text = re.sub(\"[\\?|\\!]\", 'EXPRESSION', text)\n",
    "            \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if remove_char:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "            \n",
    "        #REMOVE TAGS\n",
    "        if remove_tag:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            if english:\n",
    "                lemma = WordNetLemmatizer()\n",
    "                text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "#             else:\n",
    "#                 lemma = libvoikko.Voikko(u\"fi\")\n",
    "#                 text = \" \".join(lemma.analyze(word)[0]['BASEFORM'] for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(stem.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def clean_zh_stopwords(text_list, stopwords_set='merged'):\n",
    "    \"\"\"\n",
    "    Function that removes chinese stopwords\n",
    "    \n",
    "    :param stopwords_set: remove words of both sets (merged), just the 1st (fst) or just the second (snd) \n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    \n",
    "    zh_stopwords1 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords1.txt', 'r', encoding='utf-8').readlines()]\n",
    "    zh_stopwords2 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords2.txt', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    if stopwords_set == 'merged':\n",
    "        stop = list(set(zh_stopwords1 + zh_stopwords2))\n",
    "    elif stopwords_set == 'fst':\n",
    "        stop = zh_stopwords1\n",
    "    elif stopwords_set == 'snd':\n",
    "        stop = zh_stopwords2\n",
    "        \n",
    "\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        text = text_list[j]\n",
    "        text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "        \n",
    "    \n",
    "def update_df(dataframe, list_updated):\n",
    "    dataframe.update(pd.DataFrame({\"Text\": list_updated}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c8a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b22e4fcbff4453193c22c1b808d7618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8719ca070e1740d597dc6edbd051147a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_cleaned = pd.DataFrame()\n",
    "english_cleaned['avg-score'] = english['avg-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    english_cleaned[column] = clean(english[column], lower = True, \n",
    "                                                    remove_char = True,\n",
    "                                                    remove_stop = True,\n",
    "                                                    lemmatize = True,\n",
    "                                                    stemmer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d55d1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg-score</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>grab weapon forearm shoulder hit face free elbow</td>\n",
       "      <td>grasp gun forearm shoulder hitting face free e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>new york changed also rediscovery</td>\n",
       "      <td>new york change also reinvention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.965000</td>\n",
       "      <td>thinking summer improve give depth need get hi...</td>\n",
       "      <td>thought long hard course summer might improve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905000</td>\n",
       "      <td>find another way cheat somewhere</td>\n",
       "      <td>find another way defraud others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.746667</td>\n",
       "      <td>report replacement president administration he...</td>\n",
       "      <td>news replacement top president office come sur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg-score                                          reference  \\\n",
       "0   0.600000   grab weapon forearm shoulder hit face free elbow   \n",
       "1   0.440000                  new york changed also rediscovery   \n",
       "2   0.965000  thinking summer improve give depth need get hi...   \n",
       "3   0.905000                   find another way cheat somewhere   \n",
       "4   0.746667  report replacement president administration he...   \n",
       "\n",
       "                                         translation  \n",
       "0  grasp gun forearm shoulder hitting face free e...  \n",
       "1                   new york change also reinvention  \n",
       "2  thought long hard course summer might improve ...  \n",
       "3                    find another way defraud others  \n",
       "4  news replacement top president office come sur...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76568a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ca5894cd4f4e3bb0bad8aff9691838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6748 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f42f486943441a9bf7c544fbf568d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6748 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg-score</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3420</td>\n",
       "      <td>voit muuttaa itsesi ananasta  koirasta tai roy...</td>\n",
       "      <td>voit muuttaa itsesi ananakseksi  koiraksi tai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5840</td>\n",
       "      <td>my s ammuttiin kolme miest   kaksi    vuotiait...</td>\n",
       "      <td>my s kolmea miest  ammuttiin  kahta    vuotias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7460</td>\n",
       "      <td>tiedot tallennetaan kassakoneisiin joka tapauk...</td>\n",
       "      <td>tiedot kuitenkin tallentuvat kassoilla joka ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5360</td>\n",
       "      <td>xinhua kertoo  ett  xinyin n ytteest  oli sunn...</td>\n",
       "      <td>xinhua kertoo  ett  xinyin sunnuntaina antamas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3225</td>\n",
       "      <td>voitaisiin kuulla cbd  n kommenttitiimin toimi...</td>\n",
       "      <td>macdonaldin  joka tuli cbc n selostajatiimiin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg-score                                          reference  \\\n",
       "0     0.3420  voit muuttaa itsesi ananasta  koirasta tai roy...   \n",
       "1     0.5840  my s ammuttiin kolme miest   kaksi    vuotiait...   \n",
       "2     0.7460  tiedot tallennetaan kassakoneisiin joka tapauk...   \n",
       "3     0.5360  xinhua kertoo  ett  xinyin n ytteest  oli sunn...   \n",
       "4     0.3225  voitaisiin kuulla cbd  n kommenttitiimin toimi...   \n",
       "\n",
       "                                         translation  \n",
       "0  voit muuttaa itsesi ananakseksi  koiraksi tai ...  \n",
       "1  my s kolmea miest  ammuttiin  kahta    vuotias...  \n",
       "2  tiedot kuitenkin tallentuvat kassoilla joka ta...  \n",
       "3  xinhua kertoo  ett  xinyin sunnuntaina antamas...  \n",
       "4  macdonaldin  joka tuli cbc n selostajatiimiin ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finnish_cleaned = pd.DataFrame()\n",
    "finnish_cleaned['avg-score'] = finnish['avg-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    finnish_cleaned[column] = clean(finnish[column], lower = True, \n",
    "                                                    remove_char = True,\n",
    "                                                    lemmatize = True,\n",
    "                                                    stemmer = False,\n",
    "                                                    english=False)\n",
    "finnish_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2faed125",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chinese_stopwords/chinese_stopwords1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7276f451ff6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mchinese_cleaned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'avg-score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchinese\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'avg-score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'reference'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'translation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mchinese_cleaned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_zh_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchinese\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mchinese_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9d7887097524>\u001b[0m in \u001b[0;36mclean_zh_stopwords\u001b[1;34m(text_list, stopwords_set)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mzh_stopwords1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chinese_stopwords/chinese_stopwords1.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mzh_stopwords2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chinese_stopwords/chinese_stopwords2.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chinese_stopwords/chinese_stopwords1.txt'"
     ]
    }
   ],
   "source": [
    "chinese_cleaned = pd.DataFrame()\n",
    "chinese_cleaned['avg-score'] = chinese['avg-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    chinese_cleaned[column] = clean_zh_stopwords(chinese[column])\n",
    "chinese_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e63d2",
   "metadata": {
    "id": "NjBupUyDLNRa"
   },
   "source": [
    "# Train, Dev & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5d3aa6",
   "metadata": {
    "id": "qZKkQkBNLSYT"
   },
   "outputs": [],
   "source": [
    "en_train, en_dev = train_test_split(english_cleaned, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev, en_test = train_test_split(en_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "##NEED TO IMPLEMENT\n",
    "# fin_train, fin_dev = train_test_split(finnish, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "# fin_dev, fin_test = train_test_split(fin_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "# ch_train, ch_dev = train_test_split(chinese, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "# ch_dev, ch_test = train_test_split(ch_dev, shuffle = True, test_size = 0.5, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4188f32",
   "metadata": {},
   "source": [
    "# Encoding (for now only bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "451604d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CountVectorizer()\n",
    "names = ['en_train', 'en_dev', 'en_test']\n",
    "for i,df in enumerate([en_train, en_dev, en_test]):\n",
    "    for column in ['reference', 'translation']:\n",
    "        encoded_df = names[i] + '_encoded_' + column\n",
    "        if i == 0:\n",
    "            vars()[encoded_df] = encoder.fit_transform(df[column]).todense()\n",
    "        else:\n",
    "            vars()[encoded_df] = encoder.transform(df[column]).todense()\n",
    "            \n",
    "    y_name = 'y_' + names[i].split('_')[1]\n",
    "    vars()[y_name] = np.array(df['avg-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740c2db",
   "metadata": {},
   "source": [
    "# Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de2f0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dev = []\n",
    "for i in range(en_dev_encoded_reference.shape[0]):\n",
    "    cos_dev.append(cosine_similarity(en_dev_encoded_reference[i], en_dev_encoded_translation[i])[0])\n",
    "cos_dev = np.array(cos_dev)\n",
    "cos_dev.shape = (cos_dev.shape[0],)\n",
    "\n",
    "cos_test = []\n",
    "for i in range(en_test_encoded_reference.shape[0]):\n",
    "    cos_test.append(cosine_similarity(en_test_encoded_reference[i], en_test_encoded_translation[i])[0])\n",
    "cos_test = np.array(cos_test)\n",
    "cos_test.shape = (cos_test.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e00d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between cosine similarity and score on development set: 0.27125392707842255 (p-value < 0.001: True); and Kendall Tau: 0.1941738794482738 (p-value < 0.001: True)\n",
      "Pearson correlation between cosine similarity and score on test set: 0.26633356530702623 (p-value < 0.001: True); and Kendall Tau: 0.18264304507499624 (p-value < 0.001: True)\n"
     ]
    }
   ],
   "source": [
    "cleaned_corr_dev, cleaned_corr_dev_pvalue = pearsonr(y_dev, cos_dev)\n",
    "cleaned_corr_test, cleaned_corr_test_pvalue = pearsonr(y_test,cos_test)\n",
    "\n",
    "cleaned_corr_ktau_dev, cleaned_corr_ktau_dev_pvalue = kendalltau(y_dev, cos_dev)\n",
    "cleaned_corr_ktau_test, cleaned_corr_ktau_test_pvalue = kendalltau(y_test,cos_test)\n",
    "\n",
    "print(f'Pearson correlation between cosine similarity and score on development set: {cleaned_corr_dev} (p-value < 0.001: {cleaned_corr_dev_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_dev} (p-value < 0.001: {cleaned_corr_ktau_dev_pvalue < 0.001})')\n",
    "print(f'Pearson correlation between cosine similarity and score on test set: {cleaned_corr_test} (p-value < 0.001: {cleaned_corr_test_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_test} (p-value < 0.001: {cleaned_corr_ktau_test_pvalue < 0.001})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
