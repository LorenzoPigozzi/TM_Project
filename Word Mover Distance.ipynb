{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1705b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873d7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce514b72",
   "metadata": {
    "id": "jA_9c3SpLTUn"
   },
   "source": [
    "# Data Importing and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb58d83f",
   "metadata": {
    "id": "nU_gOCWtF8XY"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('corpus')\n",
    "files.remove('.DS_Store')\n",
    "files.remove('scores_ru-en.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for file_ in files:\n",
    "  name = file_.split('-')[0] + file_.split('-')[1]\n",
    "  vars()[name] = pd.read_csv(os.path.join('corpus', file_, 'scores.csv'))\n",
    "  vars()[name].drop(columns = ['source', 'annotators', 'z-score'], inplace = True)\n",
    "  vars()[name]['avg-score'] = scaler.fit_transform(vars()[name]['avg-score'].values.reshape(-1,1)) #normalizing values betwewen 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1ef1c2",
   "metadata": {
    "id": "EcUEx6elHvwd"
   },
   "outputs": [],
   "source": [
    "english = csen.copy()\n",
    "for df in [deen, ruen, zhen]:\n",
    "  english = english.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074be1cf",
   "metadata": {
    "id": "8ziEKCv4LFwE"
   },
   "outputs": [],
   "source": [
    "finnish = enfi.copy()\n",
    "chinese = enzh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ddc2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "english.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c2d59",
   "metadata": {},
   "source": [
    "# Cleaning the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50996387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          lower = False,\n",
    "          remove_char = False,\n",
    "          remove_stop = False,\n",
    "          lemmatize = False,\n",
    "          stemmer = False,\n",
    "          split_list = False):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    \n",
    "    updates = []\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "        \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if remove_char:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(snowball_stemmer.stem(word) for word in text.split())\n",
    "        \n",
    "        #SPLITTING THE TEXT INTO A LIST OF WORDS\n",
    "        if split_list:\n",
    "            text = text.split()\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def update_df(dataframe, list_updated):\n",
    "    dataframe.update(pd.DataFrame({\"Text\": list_updated}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c8a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a672e779cf4c7baa5a8e77691a3748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546f12e5aa28463e88edfe4cbff6bb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_cleaned = pd.DataFrame()\n",
    "english_cleaned['avg-score'] = english['avg-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    english_cleaned[column] = clean(english[column], lower = True, \n",
    "                                                    remove_char = True,\n",
    "                                                    remove_stop = True,\n",
    "                                                    lemmatize = True,\n",
    "                                                    stemmer = False,\n",
    "                                                    split_list = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d55d1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg-score</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>[grab, weapon, forearm, shoulder, hit, face, f...</td>\n",
       "      <td>[grasp, gun, forearm, shoulder, hitting, face,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>[new, york, changed, also, rediscovery]</td>\n",
       "      <td>[new, york, change, also, reinvention]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.965000</td>\n",
       "      <td>[thinking, summer, improve, give, depth, need,...</td>\n",
       "      <td>[thought, long, hard, course, summer, might, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905000</td>\n",
       "      <td>[find, another, way, cheat, somewhere]</td>\n",
       "      <td>[find, another, way, defraud, others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.746667</td>\n",
       "      <td>[report, replacement, president, administratio...</td>\n",
       "      <td>[news, replacement, top, president, office, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg-score                                          reference  \\\n",
       "0   0.600000  [grab, weapon, forearm, shoulder, hit, face, f...   \n",
       "1   0.440000            [new, york, changed, also, rediscovery]   \n",
       "2   0.965000  [thinking, summer, improve, give, depth, need,...   \n",
       "3   0.905000             [find, another, way, cheat, somewhere]   \n",
       "4   0.746667  [report, replacement, president, administratio...   \n",
       "\n",
       "                                         translation  \n",
       "0  [grasp, gun, forearm, shoulder, hitting, face,...  \n",
       "1             [new, york, change, also, reinvention]  \n",
       "2  [thought, long, hard, course, summer, might, i...  \n",
       "3              [find, another, way, defraud, others]  \n",
       "4  [news, replacement, top, president, office, co...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e63d2",
   "metadata": {
    "id": "NjBupUyDLNRa"
   },
   "source": [
    "# Train, Dev & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5d3aa6",
   "metadata": {
    "id": "qZKkQkBNLSYT"
   },
   "outputs": [],
   "source": [
    "en_train, en_dev = train_test_split(english_cleaned, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev, en_test = train_test_split(en_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "##NEED TO IMPLEMENT\n",
    "# fin_train, fin_dev = train_test_split(finnish, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "# fin_dev, fin_test = train_test_split(fin_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "# ch_train, ch_dev = train_test_split(chinese, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "# ch_dev, ch_test = train_test_split(ch_dev, shuffle = True, test_size = 0.5, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4188f32",
   "metadata": {},
   "source": [
    "# Encoding (Word2Vec + Word Mover Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "451604d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610a3951ac064f1e903d7446ee0d5d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e75b6e779444649eb9b29bc4aaf34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7412761db5784fafbfa7286b1c21cef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = api.load('word2vec-google-news-300')\n",
    "names = ['en_train', 'en_dev', 'en_test']\n",
    "for j,df in enumerate([en_train, en_dev, en_test]):\n",
    "    name = 'distances_' + names[j]\n",
    "    vars()[name] = []\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    for i in tqdm(range(len(df))):\n",
    "                  \n",
    "        vars()[name].append(model.wmdistance(df['reference'][i], df['translation'][i]))\n",
    "    \n",
    "    name2 = 'score_' + names[j]\n",
    "    vars()[name2] = np.array(df['avg-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3eff71",
   "metadata": {},
   "source": [
    "# Calculating correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e00d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_dev_true, y_dev_pred, y_test_true = None, y_test_pred = None, return_corr = False):\n",
    "\n",
    "    cleaned_corr_dev, cleaned_corr_dev_pvalue = pearsonr(y_dev_true, y_dev_pred)\n",
    "    cleaned_corr_ktau_test, cleaned_corr_ktau_test_pvalue = kendalltau(y_test_true, y_test_pred)\n",
    "\n",
    "    if y_test_true != None:\n",
    "        cleaned_corr_ktau_dev, cleaned_corr_ktau_dev_pvalue = kendalltau(y_dev_true, y_dev_pred)\n",
    "        cleaned_corr_test, cleaned_corr_test_pvalue = pearsonr(y_test_true, y_test_pred)\n",
    "        \n",
    "\n",
    "    print(f'Pearson correlation between cosine similarity and score on development set: {cleaned_corr_dev} (p-value < 0.001: {cleaned_corr_dev_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_dev} (p-value < 0.001: {cleaned_corr_ktau_dev_pvalue < 0.001})')\n",
    "    print(f'Pearson correlation between cosine similarity and score on test set: {cleaned_corr_test} (p-value < 0.001: {cleaned_corr_test_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_test} (p-value < 0.001: {cleaned_corr_ktau_test_pvalue < 0.001})')\n",
    "    \n",
    "    if return_corr:\n",
    "        return cleaned_corr_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7f24a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.inf in distances_en_dev:\n",
    "    distances_en_dev[distances_en_dev.index(np.inf)] = 10\n",
    "    \n",
    "while np.inf in distances_en_test:\n",
    "    distances_en_test[distances_en_test.index(np.inf)] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "181f3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between cosine similarity and score on development set: -0.19745565618100078 (p-value < 0.001: True); and Kendall Tau: -0.21976558377750754 (p-value < 0.001: True)\n",
      "Pearson correlation between cosine similarity and score on test set: -0.217635667687527 (p-value < 0.001: True); and Kendall Tau: -0.20764484095340793 (p-value < 0.001: True)\n"
     ]
    }
   ],
   "source": [
    "corr(score_en_dev, distances_en_dev, score_en_test, distances_en_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
