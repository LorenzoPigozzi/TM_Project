{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1a8e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fa7ae5588254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkendalltau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c00b3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314482 sha256=8e27de8805154b8b927a53612e7556c4554f0591c0415f37efb6175230824201\n",
      "  Stored in directory: c:\\users\\luisl\\appdata\\local\\pip\\cache\\wheels\\ca\\38\\d8\\dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bddf2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a21b4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe017d7c",
   "metadata": {
    "id": "jA_9c3SpLTUn"
   },
   "source": [
    "# Data Importing and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06baf15b",
   "metadata": {
    "id": "nU_gOCWtF8XY"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('corpus')\n",
    "files.remove('.DS_Store')\n",
    "files.remove('scores_ru-en.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for file_ in files:\n",
    "  name = file_.split('-')[0] + file_.split('-')[1]\n",
    "  vars()[name] = pd.read_csv(os.path.join('corpus', file_, 'scores.csv'))\n",
    "  vars()[name].drop(columns = ['source', 'annotators', 'avg-score'], inplace = True)\n",
    "  vars()[name]['z-score'] = scaler.fit_transform(vars()[name]['z-score'].values.reshape(-1,1)) #normalizing values betwewen 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96384c74",
   "metadata": {
    "id": "EcUEx6elHvwd"
   },
   "outputs": [],
   "source": [
    "english = csen.copy()\n",
    "for df in [deen, ruen, zhen]:\n",
    "  english = english.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15d072be",
   "metadata": {
    "id": "8ziEKCv4LFwE"
   },
   "outputs": [],
   "source": [
    "finnish = enfi.copy()\n",
    "chinese = enzh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17acc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "english.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c17ec",
   "metadata": {},
   "source": [
    "# Cleaning the corpus (updated  cleaning function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd0c8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          lower = False,\n",
    "          keep_numbers = False,\n",
    "          keep_expression = False,\n",
    "          remove_char = False,\n",
    "          remove_stop = False,\n",
    "          remove_tag = False,\n",
    "          lemmatize = False,\n",
    "          stemmer = False,\n",
    "          english = False\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    if english:\n",
    "        lang = 'english'\n",
    "    else:\n",
    "        lang = 'finnish'\n",
    "    \n",
    "    stop = set(stopwords.words(lang))\n",
    "    stem = SnowballStemmer(lang)\n",
    "    \n",
    "    updates = []\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "            \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if not keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        #KEEP '?' and '!' AS TOKENS\n",
    "        if not keep_expression:\n",
    "            text = re.sub(\"[\\?|\\!]\", 'EXPRESSION', text)\n",
    "            \n",
    "        #REMOVE TAGS\n",
    "        if remove_tag:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "            \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if remove_char:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            if english:\n",
    "                lemma = WordNetLemmatizer()\n",
    "                text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(stem.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def clean_ch(text_list, keep_numbers=False, remove_punctuation=False, remove_stop = False, stopwords_set='merged'):\n",
    "    \"\"\"\n",
    "    Function that removes chinese stopwords\n",
    "    \n",
    "    :param stopwords_set: remove words of both sets (merged), just the 1st (fst) or just the second (snd) \n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    \n",
    "    zh_stopwords1 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords1.txt', 'r', encoding='utf-8').readlines()]\n",
    "    zh_stopwords2 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords2.txt', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    if stopwords_set == 'merged':\n",
    "        stop = list(set(zh_stopwords1 + zh_stopwords2))\n",
    "    elif stopwords_set == 'fst':\n",
    "        stop = zh_stopwords1\n",
    "    elif stopwords_set == 'snd':\n",
    "        stop = zh_stopwords2\n",
    "\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        # REMOVE PUNCTUATION\n",
    "        if remove_punctuation:\n",
    "            # https://stackoverflow.com/questions/36640587/how-to-remove-chinese-punctuation-in-python\n",
    "            punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "            text = re.sub(r\"[%s]+\" %punc, \"\", text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            pretext = text\n",
    "            text = ' '.join([word for word in jieba.cut(text) if word not in stop])\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef630f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Temp\\jieba.cache\n",
      "Loading model cost 0.658 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# En\n",
    "cleaning_dict = {'lower': False, 'keep_numbers': True, 'keep_expression': False, 'remove_char': True, 'remove_stop': True, 'remove_tag': False, 'lemmatize': False, 'stemmer': True}\n",
    "english_clean = pd.DataFrame()\n",
    "english_clean['z-score'] = english['z-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    english_clean[column] = clean(english[column], cleaning_dict)\n",
    "    \n",
    "# Fi\n",
    "finnish_clean = pd.DataFrame()\n",
    "for c in ['reference', 'translation']:\n",
    "    finnish_clean[c] = clean(finnish[c],\n",
    "                             lower = False,\n",
    "                             keep_numbers = False,\n",
    "                             keep_expression = True,\n",
    "                             remove_char = True,\n",
    "                             remove_stop = False,\n",
    "                             remove_tag = True,\n",
    "                             lemmatize = False,\n",
    "                             stemmer = True,\n",
    "                             english=False)\n",
    "finnish_clean['z-score'] = finnish['z-score']\n",
    "\n",
    "#Ch\n",
    "chinese_clean = pd.DataFrame()\n",
    "for c in ['reference', 'translation']:\n",
    "    chinese_clean[c] = clean_ch(chinese[c],\n",
    "                                keep_numbers = False,\n",
    "                                remove_punctuation = False,\n",
    "                                remove_stop = True,\n",
    "                                stopwords_set = 'snd')\n",
    "chinese_clean['z-score'] = chinese['z-score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a3bc0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616c3d6b33ff45f3bdcc26b5806b04cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=77688.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9420f84991eb40e49baf926256b9241c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=77688.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning_dict = {'lower': False, 'keep_numbers': True, 'keep_expression': False, 'remove_char': True, 'remove_stop': True, 'remove_tag': False, 'lemmatize': False, 'stemmer': True}\n",
    "# english_cleaned = pd.DataFrame()\n",
    "# english_cleaned['z-score'] = english['z-score']\n",
    "# for column in ['reference', 'translation']:\n",
    "#     english_cleaned[column] = clean(english[column], cleaning_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a1a4a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1371b438694eb0a950baa808533746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdabf2b23b5e42038943b8aef92a1d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6748.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning_dict = {'lower': False, 'keep_numbers': True, 'keep_expression': False, 'remove_char': True, 'remove_stop': True, 'remove_tag': False, 'lemmatize': False, 'stemmer': True}\n",
    "# finnish_cleaned = pd.DataFrame()\n",
    "# finnish_cleaned['z-score'] = finnish['z-score']\n",
    "# for column in ['reference', 'translation']:\n",
    "#     finnish_cleaned[column] = clean(finnish[column], cleaning_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa76dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0230b40ed86c402fbdf72f1f099babe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a26952228a14cdabf5b6b9d498f3d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning_dict = {'lower': False, 'keep_numbers': True, 'keep_expression': False, 'remove_char': True, 'remove_stop': True, 'remove_tag': False, 'lemmatize': False, 'stemmer': True}\n",
    "# chinese_cleaned = pd.DataFrame()\n",
    "# chinese_cleaned['z-score'] = chinese['z-score']\n",
    "# for column in ['reference', 'translation']:\n",
    "#     chinese_cleaned[column] = clean(chinese[column], cleaning_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b1f83",
   "metadata": {
    "id": "NjBupUyDLNRa"
   },
   "source": [
    "# Train, Dev & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8d0830c",
   "metadata": {
    "id": "qZKkQkBNLSYT"
   },
   "outputs": [],
   "source": [
    "# English\n",
    "en_train, en_dev = train_test_split(english, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev, en_test = train_test_split(en_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "en_train_clean, en_dev_clean = train_test_split(english_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev_clean, en_test_clean = train_test_split(en_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "\n",
    "# Finnish\n",
    "fin_train, fin_dev = train_test_split(finnish, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "fin_dev, fin_test = train_test_split(fin_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "fin_train_clean, fin_dev_clean = train_test_split(finnish_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "fin_dev_clean, fin_test_clean = train_test_split(fin_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "#Chinese\n",
    "ch_train, ch_dev = train_test_split(chinese, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "ch_dev, ch_test = train_test_split(ch_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "ch_train_clean, ch_dev_clean = train_test_split(chinese_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "ch_dev_clean, ch_test_clean = train_test_split(ch_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3d238",
   "metadata": {},
   "source": [
    "# Not cleaned corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835f718",
   "metadata": {},
   "source": [
    "## Encoding (Word2Vec + Word Mover Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f310281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b9987cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984a18f9400d46ebb6fc1b729d5ea815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44917ebecd584dff95d2fb8f91bd0320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c65a9cb8ce4f22af640aa9cded2bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = ['en_train', 'en_dev', 'en_test']\n",
    "for j,df in enumerate([en_train, en_dev, en_test]):\n",
    "    name = 'distances_' + names[j]\n",
    "    vars()[name] = []\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    for i in tqdm(range(len(df))):\n",
    "                  \n",
    "        vars()[name].append(model.wmdistance(df['reference'][i], df['translation'][i]))\n",
    "    \n",
    "    name2 = 'score_' + names[j]\n",
    "    vars()[name2] = np.array(df['z-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fe9b5",
   "metadata": {},
   "source": [
    "## Calculating correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6734f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_train_true, y_train_pred, y_dev_true, y_dev_pred, y_test_true, y_test_pred, return_corr = False):\n",
    "    \n",
    "    cleaned_corr_train, cleaned_corr_train_pvalue = pearsonr(y_train_true, y_train_pred)\n",
    "    cleaned_corr_ktau_train, cleaned_corr_ktau_train_pvalue = kendalltau(y_train_true, y_train_pred)\n",
    "    \n",
    "    cleaned_corr_dev, cleaned_corr_dev_pvalue = pearsonr(y_dev_true, y_dev_pred)\n",
    "    cleaned_corr_ktau_dev, cleaned_corr_ktau_dev_pvalue = kendalltau(y_dev_true, y_dev_pred)\n",
    "\n",
    "\n",
    "    cleaned_corr_ktau_test, cleaned_corr_ktau_test_pvalue = kendalltau(y_test_true, y_test_pred)\n",
    "    cleaned_corr_test, cleaned_corr_test_pvalue = pearsonr(y_test_true, y_test_pred)\n",
    "        \n",
    "    print(f'Pearson correlation between cosine similarity and score on train set: {cleaned_corr_train} (p-value < 0.001: {cleaned_corr_train_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_train} (p-value < 0.001: {cleaned_corr_ktau_train_pvalue < 0.001})')\n",
    "    print(f'Pearson correlation between cosine similarity and score on development set: {cleaned_corr_dev} (p-value < 0.001: {cleaned_corr_dev_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_dev} (p-value < 0.001: {cleaned_corr_ktau_dev_pvalue < 0.001})')\n",
    "    print(f'Pearson correlation between cosine similarity and score on test set: {cleaned_corr_test} (p-value < 0.001: {cleaned_corr_test_pvalue < 0.001}); and Kendall Tau: {cleaned_corr_ktau_test} (p-value < 0.001: {cleaned_corr_ktau_test_pvalue < 0.001})')\n",
    "    \n",
    "    if return_corr:\n",
    "        return cleaned_corr_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31ea45",
   "metadata": {},
   "source": [
    "How to treat np.inf???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c612bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.inf in distances_en_train:\n",
    "    distances_en_train[distances_en_train.index(np.inf)] = 10\n",
    "\n",
    "while np.inf in distances_en_dev:\n",
    "    distances_en_dev[distances_en_dev.index(np.inf)] = 10\n",
    "    \n",
    "while np.inf in distances_en_test:\n",
    "    distances_en_test[distances_en_test.index(np.inf)] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4812a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between cosine similarity and score on train set: -0.2010915969716781 (p-value < 0.001: True); and Kendall Tau: -0.1847216372580606 (p-value < 0.001: True)\n",
      "Pearson correlation between cosine similarity and score on development set: -0.26042741904000105 (p-value < 0.001: True); and Kendall Tau: -0.1772273081097246 (p-value < 0.001: True)\n",
      "Pearson correlation between cosine similarity and score on test set: -0.2731128169278083 (p-value < 0.001: True); and Kendall Tau: -0.18631107517421996 (p-value < 0.001: True)\n"
     ]
    }
   ],
   "source": [
    "corr(score_en_train, distances_en_train, score_en_dev, distances_en_dev, score_en_test, distances_en_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e47e",
   "metadata": {},
   "source": [
    "# Cleaned corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e8810",
   "metadata": {},
   "source": [
    "## Encoding (Word2Vec + Word Mover Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "856aed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce20a6f46ebc43c2a0be56c43be88a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edb993ec3ce4fbab9fa4e2981cfaae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b960a92e8148c0b5017138cf0aefe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = api.load('word2vec-google-news-300')\n",
    "names = ['en_train_cleaned', 'en_dev_cleaned', 'en_test_cleaned']\n",
    "for j,df in enumerate([en_train_cleaned, en_dev_cleaned, en_test_cleaned]):\n",
    "    name = 'distances_' + names[j]\n",
    "    vars()[name] = []\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    for i in tqdm(range(len(df))):\n",
    "                  \n",
    "        vars()[name].append(model.wmdistance(df['reference'][i], df['translation'][i]))\n",
    "    \n",
    "    name2 = 'score_' + names[j]\n",
    "    vars()[name2] = np.array(df['z-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df31e74",
   "metadata": {},
   "source": [
    "## Calculating correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ed1dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.inf in distances_en_train_cleaned:\n",
    "    distances_en_train_cleaned[distances_en_train_cleaned.index(np.inf)] = 10\n",
    "\n",
    "while np.inf in distances_en_dev_cleaned:\n",
    "    distances_en_dev_cleaned[distances_en_dev_cleaned.index(np.inf)] = 10\n",
    "    \n",
    "while np.inf in distances_en_test_cleaned:\n",
    "    distances_en_test_cleaned[distances_en_test_cleaned.index(np.inf)] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b2e82a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between cosine similarity and score on train set: -0.1955674998038151 (p-value < 0.001: True); and Kendall Tau: -0.17732545076194817 (p-value < 0.001: True)\n",
      "Pearson correlation between cosine similarity and score on development set: -0.2634434105536765 (p-value < 0.001: True); and Kendall Tau: -0.17342002949899588 (p-value < 0.001: True)\n",
      "Pearson correlation between cosine similarity and score on test set: -0.2731052532305561 (p-value < 0.001: True); and Kendall Tau: -0.18236889501446313 (p-value < 0.001: True)\n"
     ]
    }
   ],
   "source": [
    "corr(score_en_train_cleaned, distances_en_train_cleaned, score_en_dev_cleaned, distances_en_dev_cleaned, score_en_test_cleaned, distances_en_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfb11465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSIS的科学家AnthonyDelGenio在新闻稿中解释说：“在GISS模型的模拟模型中...</td>\n",
       "      <td>戈达德太空研究所科学家安东尼·德尔·杰尼奥在新闻发布会上解释说：“在戈达德太空研究所的模型模...</td>\n",
       "      <td>0.451621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中国在英国女性4x200mFreestreyWTE中的最后被称为：“中国14岁的孩子从球下降...</td>\n",
       "      <td>参加女子4x200米自由泳接力赛决赛的中国小将艾衍含被这样描述：“那名14岁的中国小姑娘犯了...</td>\n",
       "      <td>0.246545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>然后来到2012年，当她和她的队友们没有什么好处。</td>\n",
       "      <td>2012年，她和她的队友都不被看好。</td>\n",
       "      <td>0.198549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>自去年以来，GoudianGroup从南非通过南非港口出口了163套风力发电项目。</td>\n",
       "      <td>自去年以来，国电集团共计有163套风电项目陆续从连云港港出口南非。</td>\n",
       "      <td>0.216002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>一些人指称，Kempinski旅馆只是\"被捕\"，以满足阿拉伯客户的要求。</td>\n",
       "      <td>有人认为凯宾斯基酒店简直是为了满足阿拉伯客户的要求而“卑躬屈膝”。</td>\n",
       "      <td>0.391471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10216</th>\n",
       "      <td>这不是一个大型的会议，它不是皇家 Ascot -它是一个普通的星期五晚上，有20，000人。</td>\n",
       "      <td>这不是一场大规模的赛马会，也不是英国皇家爱斯科赛马会 (Royal Ascot)，这只是一个...</td>\n",
       "      <td>0.688151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>这位负责人强调，钢铁产品的国际贸易本质上是一种市场行为，是从进口国的要求和考虑产品成本和其他...</td>\n",
       "      <td>这位负责人强调，钢铁产品国际贸易本质上讲是市场行为，是源于进口国需求、消费者综合考虑产品性价...</td>\n",
       "      <td>0.817705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>她继续浏览，并最终在 Bouverie 广场购物中心的商店里尝试了四双内衣。</td>\n",
       "      <td>她继续浏览货架，并最终在位于 Folkestone 的 Bouverie Place 购物中...</td>\n",
       "      <td>0.711189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>但是当他带着女儿参加综艺节目“爸爸，我们去哪里？”时，他成为了一个焦点。</td>\n",
       "      <td>但在2004年奥运会单人十米台比赛中输给队友胡佳后，田亮的竞技状态出现下滑，并被爆出被跳水队开除。</td>\n",
       "      <td>0.222476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10220</th>\n",
       "      <td>行业数据显示，2015年煤炭生产总量接近60亿吨，全国煤炭生产产能过剩达到18亿吨。</td>\n",
       "      <td>行业数据显示，2015年煤炭产能总规模接近60亿吨，全国煤炭产能过剩18亿吨。</td>\n",
       "      <td>0.903920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10221 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reference  \\\n",
       "0      GSIS的科学家AnthonyDelGenio在新闻稿中解释说：“在GISS模型的模拟模型中...   \n",
       "1      中国在英国女性4x200mFreestreyWTE中的最后被称为：“中国14岁的孩子从球下降...   \n",
       "2                              然后来到2012年，当她和她的队友们没有什么好处。   \n",
       "3              自去年以来，GoudianGroup从南非通过南非港口出口了163套风力发电项目。   \n",
       "4                   一些人指称，Kempinski旅馆只是\"被捕\"，以满足阿拉伯客户的要求。   \n",
       "...                                                  ...   \n",
       "10216     这不是一个大型的会议，它不是皇家 Ascot -它是一个普通的星期五晚上，有20，000人。   \n",
       "10217  这位负责人强调，钢铁产品的国际贸易本质上是一种市场行为，是从进口国的要求和考虑产品成本和其他...   \n",
       "10218             她继续浏览，并最终在 Bouverie 广场购物中心的商店里尝试了四双内衣。   \n",
       "10219               但是当他带着女儿参加综艺节目“爸爸，我们去哪里？”时，他成为了一个焦点。   \n",
       "10220         行业数据显示，2015年煤炭生产总量接近60亿吨，全国煤炭生产产能过剩达到18亿吨。   \n",
       "\n",
       "                                             translation   z-score  \n",
       "0      戈达德太空研究所科学家安东尼·德尔·杰尼奥在新闻发布会上解释说：“在戈达德太空研究所的模型模...  0.451621  \n",
       "1      参加女子4x200米自由泳接力赛决赛的中国小将艾衍含被这样描述：“那名14岁的中国小姑娘犯了...  0.246545  \n",
       "2                                     2012年，她和她的队友都不被看好。  0.198549  \n",
       "3                      自去年以来，国电集团共计有163套风电项目陆续从连云港港出口南非。  0.216002  \n",
       "4                      有人认为凯宾斯基酒店简直是为了满足阿拉伯客户的要求而“卑躬屈膝”。  0.391471  \n",
       "...                                                  ...       ...  \n",
       "10216  这不是一场大规模的赛马会，也不是英国皇家爱斯科赛马会 (Royal Ascot)，这只是一个...  0.688151  \n",
       "10217  这位负责人强调，钢铁产品国际贸易本质上讲是市场行为，是源于进口国需求、消费者综合考虑产品性价...  0.817705  \n",
       "10218  她继续浏览货架，并最终在位于 Folkestone 的 Bouverie Place 购物中...  0.711189  \n",
       "10219  但在2004年奥运会单人十米台比赛中输给队友胡佳后，田亮的竞技状态出现下滑，并被爆出被跳水队开除。  0.222476  \n",
       "10220            行业数据显示，2015年煤炭产能总规模接近60亿吨，全国煤炭产能过剩18亿吨。  0.903920  \n",
       "\n",
       "[10221 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45723a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>helsing s ti n haltij hank perusajatus on suoj...</td>\n",
       "      <td>helsink foundation haltij projekt perusajatuks...</td>\n",
       "      <td>0.683731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>tapaus synnyt melkois vastauks ihmist kan verk...</td>\n",
       "      <td>v likohtaus tuot melkois vastareaktio verko ku...</td>\n",
       "      <td>0.387462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>lain muutos helpot ihmist pelottelu pelottelu ...</td>\n",
       "      <td>muuta lak niin et on helpomp syyt ihm pelottel...</td>\n",
       "      <td>0.426999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>mal tarkastelt my s venuks varhais venuks tuop...</td>\n",
       "      <td>mal my s tarkastel varhais venuks pinnanmuodostu</td>\n",
       "      <td>0.279282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6322</th>\n",
       "      <td>kuite mielipidetutkimuks osoittav et on pahoin...</td>\n",
       "      <td>gallup kuite kertov et hallituspuolue perussuo...</td>\n",
       "      <td>0.183373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5699</th>\n",
       "      <td>lick lick j ti valtuustotalo x xx pun vahingo ...</td>\n",
       "      <td>yk riperh aiheut vuokratalo x xx pun vahingo s...</td>\n",
       "      <td>0.317478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>matkustaj voi valitettav lemmikkikoir pukeutun...</td>\n",
       "      <td>matkustaj eiv t valitettav voi tul silit m n t...</td>\n",
       "      <td>0.295974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>oli kerm puf tapahtum</td>\n",
       "      <td>tapahtui tuulihattuv likohtaus</td>\n",
       "      <td>0.412813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>tiete muka kaik n m asia voiva tuoda sinu vuos...</td>\n",
       "      <td>tiete muka kaik n m asia saattav tuoda lis eli...</td>\n",
       "      <td>0.900195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>thaima on kuolu v hint n nelj ihmist x tun</td>\n",
       "      <td>thaima on kuolu vuorokaud aika aina nelj ihmis...</td>\n",
       "      <td>0.112369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reference  \\\n",
       "100   helsing s ti n haltij hank perusajatus on suoj...   \n",
       "2462  tapaus synnyt melkois vastauks ihmist kan verk...   \n",
       "5630  lain muutos helpot ihmist pelottelu pelottelu ...   \n",
       "4299  mal tarkastelt my s venuks varhais venuks tuop...   \n",
       "6322  kuite mielipidetutkimuks osoittav et on pahoin...   \n",
       "...                                                 ...   \n",
       "5699  lick lick j ti valtuustotalo x xx pun vahingo ...   \n",
       "2550  matkustaj voi valitettav lemmikkikoir pukeutun...   \n",
       "537                               oli kerm puf tapahtum   \n",
       "1220  tiete muka kaik n m asia voiva tuoda sinu vuos...   \n",
       "4271         thaima on kuolu v hint n nelj ihmist x tun   \n",
       "\n",
       "                                            translation   z-score  \n",
       "100   helsink foundation haltij projekt perusajatuks...  0.683731  \n",
       "2462  v likohtaus tuot melkois vastareaktio verko ku...  0.387462  \n",
       "5630  muuta lak niin et on helpomp syyt ihm pelottel...  0.426999  \n",
       "4299   mal my s tarkastel varhais venuks pinnanmuodostu  0.279282  \n",
       "6322  gallup kuite kertov et hallituspuolue perussuo...  0.183373  \n",
       "...                                                 ...       ...  \n",
       "5699  yk riperh aiheut vuokratalo x xx pun vahingo s...  0.317478  \n",
       "2550  matkustaj eiv t valitettav voi tul silit m n t...  0.295974  \n",
       "537                      tapahtui tuulihattuv likohtaus  0.412813  \n",
       "1220  tiete muka kaik n m asia saattav tuoda lis eli...  0.900195  \n",
       "4271  thaima on kuolu vuorokaud aika aina nelj ihmis...  0.112369  \n",
       "\n",
       "[5398 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "deaf5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a573d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ch_dev_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f536f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3505</th>\n",
       "      <td>8 12 早晨 国务院 问讯 台 2016 7 举行 国民经济 操作 一次 新闻 招待会</td>\n",
       "      <td>8 12 上午 国务院新闻办 2016 7 月份 国民经济 运行 情况 举行 新闻 发布会</td>\n",
       "      <td>0.752975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>猎人 狼 :   美国 男子 射箭 队 成员 布雷 迪 埃里森 ,   左 ,   比作 莱...</td>\n",
       "      <td>猎人 狼 美国 男子 射箭 队员 布莱迪 · 埃里森   ( Brady   Ellison...</td>\n",
       "      <td>0.207275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9010</th>\n",
       "      <td>参议院 多数党 领导人 麦克 康奈尔 Mitch   McConnell 表示 一任 总统 ...</td>\n",
       "      <td>参议院 多数党 领袖 米奇 · 麦康奈尔   ( Mitch   McConnell )  ...</td>\n",
       "      <td>0.722492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>2016 可能 柏林 新 德国 首都 消灭 以色列 根除</td>\n",
       "      <td>以色列 居然 2016 德国 首都 柏林 消失</td>\n",
       "      <td>0.837772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>朱利叶 斯 ?   尼尔森     Julius   Nielsen     研究 鲨鱼  ...</td>\n",
       "      <td>一直 研究 鲨鱼   Julius   Nielsen   资料 记载 一头 雌性 小头 睡...</td>\n",
       "      <td>0.487816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>带走 重要</td>\n",
       "      <td>对手 影响 非常 重要</td>\n",
       "      <td>0.595979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>三重奏 幸运 2014 治疗 过时 保持 中断 最低 情况 正确 表示</td>\n",
       "      <td>幸运 2014 年初 治疗 影响 降到 最低 这种 情况 可能 合适</td>\n",
       "      <td>0.393515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9224</th>\n",
       "      <td>邓小飞 延误 收到 黄牌 罚球区 前面 犯规 替换 彭 新立 拿到 一张 黄牌</td>\n",
       "      <td>邓小飞 判定 拖延时间 领到 黄牌 替补 出场 彭欣力 禁区 前 犯规 领到 黄牌</td>\n",
       "      <td>0.648436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9839</th>\n",
       "      <td>黑 煤田 仓库 变成 “ 绿草如茵 ” 足球场 红砖 尖石 厂 变成 一座 现代化 体育 博物馆</td>\n",
       "      <td>黑乎乎 煤场 仓库 变成 “ 绿草如茵 ” 足球场 红砖 尖顶 厂房 变成 现代化 运动 馆</td>\n",
       "      <td>0.754224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>巧合</td>\n",
       "      <td>绝非 巧合</td>\n",
       "      <td>0.805958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1022 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reference  \\\n",
       "3505       8 12 早晨 国务院 问讯 台 2016 7 举行 国民经济 操作 一次 新闻 招待会   \n",
       "6558  猎人 狼 :   美国 男子 射箭 队 成员 布雷 迪 埃里森 ,   左 ,   比作 莱...   \n",
       "9010  参议院 多数党 领导人 麦克 康奈尔 Mitch   McConnell 表示 一任 总统 ...   \n",
       "3077                       2016 可能 柏林 新 德国 首都 消灭 以色列 根除   \n",
       "5075  朱利叶 斯 ?   尼尔森     Julius   Nielsen     研究 鲨鱼  ...   \n",
       "...                                                 ...   \n",
       "9923                                              带走 重要   \n",
       "3546                三重奏 幸运 2014 治疗 过时 保持 中断 最低 情况 正确 表示   \n",
       "9224            邓小飞 延误 收到 黄牌 罚球区 前面 犯规 替换 彭 新立 拿到 一张 黄牌   \n",
       "9839   黑 煤田 仓库 变成 “ 绿草如茵 ” 足球场 红砖 尖石 厂 变成 一座 现代化 体育 博物馆   \n",
       "3143                                                 巧合   \n",
       "\n",
       "                                            translation   z-score  \n",
       "3505      8 12 上午 国务院新闻办 2016 7 月份 国民经济 运行 情况 举行 新闻 发布会  0.752975  \n",
       "6558  猎人 狼 美国 男子 射箭 队员 布莱迪 · 埃里森   ( Brady   Ellison...  0.207275  \n",
       "9010  参议院 多数党 领袖 米奇 · 麦康奈尔   ( Mitch   McConnell )  ...  0.722492  \n",
       "3077                            以色列 居然 2016 德国 首都 柏林 消失  0.837772  \n",
       "5075  一直 研究 鲨鱼   Julius   Nielsen   资料 记载 一头 雌性 小头 睡...  0.487816  \n",
       "...                                                 ...       ...  \n",
       "9923                                        对手 影响 非常 重要  0.595979  \n",
       "3546                 幸运 2014 年初 治疗 影响 降到 最低 这种 情况 可能 合适  0.393515  \n",
       "9224          邓小飞 判定 拖延时间 领到 黄牌 替补 出场 彭欣力 禁区 前 犯规 领到 黄牌  0.648436  \n",
       "9839     黑乎乎 煤场 仓库 变成 “ 绿草如茵 ” 足球场 红砖 尖顶 厂房 变成 现代化 运动 馆  0.754224  \n",
       "3143                                              绝非 巧合  0.805958  \n",
       "\n",
       "[1022 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d198e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train[train['translation']== '']) > 0:\n",
    "    trans_list = train[train['translation']== ''].index.to_list()\n",
    "    train=train.drop(trans_list,axis=0)\n",
    "    \n",
    "if len(train[train['reference']== '']) > 0:\n",
    "    ref_list = train[train['reference']== ''].index.to_list()\n",
    "    train=train.drop(ref_list,axis=0)\n",
    "    \n",
    "     \n",
    "    \n",
    "train=train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e5e9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['rouge-1','rouge-2','rouge-l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97a1e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score(df,metric):\n",
    "    \n",
    "    \n",
    "    rouge = Rouge()\n",
    "    references =df['reference'].to_list()\n",
    "    translation =df['translation'].to_list()\n",
    "    scores=rouge.get_scores(translation, references)\n",
    "    rougedf = pd.DataFrame()\n",
    "\n",
    "    for score in scores:\n",
    "        new_row = score[metric]\n",
    "        rougedf = rougedf.append(new_row, ignore_index=True)\n",
    "    rougedf.rename(columns={'f':'F1','p':'Precision','r':'Recall'},inplace=True)\n",
    "    rougedf['Z-score']=train.iloc[:,-1]\n",
    "    \n",
    "    \n",
    "        \n",
    "    return(rougedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ac0d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation scores for rouge-1 metric \n",
      "\n",
      "F1\n",
      "Pearson correlation between RougeScore and score on development set: 0.4560311953072407 (p-value < 0.001: True); and Kendall Tau correlation: 0.31476099352769943 (p-value < 0.001: True)\n",
      "Precision\n",
      "Pearson correlation between RougeScore and score on development set: 0.45844589837035377 (p-value < 0.001: True); and Kendall Tau correlation: 0.32373343209892613 (p-value < 0.001: True)\n",
      "Recall\n",
      "Pearson correlation between RougeScore and score on development set: 0.4200236411485583 (p-value < 0.001: True); and Kendall Tau correlation: 0.28901528534576454 (p-value < 0.001: True)\n",
      "\n",
      "\n",
      "Correlation scores for rouge-2 metric \n",
      "\n",
      "F1\n",
      "Pearson correlation between RougeScore and score on development set: 0.38529887403361995 (p-value < 0.001: True); and Kendall Tau correlation: 0.2800140323240911 (p-value < 0.001: True)\n",
      "Precision\n",
      "Pearson correlation between RougeScore and score on development set: 0.38511023895221713 (p-value < 0.001: True); and Kendall Tau correlation: 0.28579130953578585 (p-value < 0.001: True)\n",
      "Recall\n",
      "Pearson correlation between RougeScore and score on development set: 0.3762315726551286 (p-value < 0.001: True); and Kendall Tau correlation: 0.2737728833662506 (p-value < 0.001: True)\n",
      "\n",
      "\n",
      "Correlation scores for rouge-l metric \n",
      "\n",
      "F1\n",
      "Pearson correlation between RougeScore and score on development set: 0.47669296002228445 (p-value < 0.001: True); and Kendall Tau correlation: 0.3322986530067027 (p-value < 0.001: True)\n",
      "Precision\n",
      "Pearson correlation between RougeScore and score on development set: 0.48186986235282486 (p-value < 0.001: True); and Kendall Tau correlation: 0.3414012576652352 (p-value < 0.001: True)\n",
      "Recall\n",
      "Pearson correlation between RougeScore and score on development set: 0.4443166648567485 (p-value < 0.001: True); and Kendall Tau correlation: 0.31062129007095624 (p-value < 0.001: True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    rougedf=rouge_score(train,metric)\n",
    "    rougedf.corr(method='pearson')\n",
    "    score = rougedf['Z-score']\n",
    "    \n",
    "    rougescore_corr, rougescore_corr_pvalue = pearsonr(score,rougedf['F1'])\n",
    "    rougescore_corr_ktau, rougescore_corr_ktau_pvalue = kendalltau(score, rougedf['F1'])\n",
    "    print('Correlation scores for',metric,'metric \\n')\n",
    "    print('F1')\n",
    "    print(f'Pearson correlation between RougeScore and score on development set: {rougescore_corr} (p-value < 0.001: {rougescore_corr_pvalue < 0.001}); and Kendall Tau correlation: {rougescore_corr_ktau} (p-value < 0.001: {rougescore_corr_ktau_pvalue < 0.001})')\n",
    "  \n",
    "    rougescore_corr, rougescore_corr_pvalue = pearsonr(score,rougedf['Precision'])\n",
    "    rougescore_corr_ktau, rougescore_corr_ktau_pvalue = kendalltau(score, rougedf['Precision'])\n",
    "    \n",
    "    print('Precision')\n",
    "    print(f'Pearson correlation between RougeScore and score on development set: {rougescore_corr} (p-value < 0.001: {rougescore_corr_pvalue < 0.001}); and Kendall Tau correlation: {rougescore_corr_ktau} (p-value < 0.001: {rougescore_corr_ktau_pvalue < 0.001})')\n",
    "    \n",
    "    \n",
    "    rougescore_corr, rougescore_corr_pvalue = pearsonr(score,rougedf['Recall'])\n",
    "    rougescore_corr_ktau, rougescore_corr_ktau_pvalue = kendalltau(score, rougedf['Recall'])\n",
    "    print('Recall')\n",
    "    print(f'Pearson correlation between RougeScore and score on development set: {rougescore_corr} (p-value < 0.001: {rougescore_corr_pvalue < 0.001}); and Kendall Tau correlation: {rougescore_corr_ktau} (p-value < 0.001: {rougescore_corr_ktau_pvalue < 0.001})')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f59f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "rougedf=rouge_score(train,'rouge-l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d4b259d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Z-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.890795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.671382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.797958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.745917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.802921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.661060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8168</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.738859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8169</th>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.800686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.449487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8171</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.653666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8172 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            F1  Precision    Recall   Z-score\n",
       "0     0.160000   0.142857  0.181818  0.890795\n",
       "1     0.400000   0.380952  0.421053  0.671382\n",
       "2     0.200000   0.200000  0.200000  0.797958\n",
       "3     0.727273   0.666667  0.800000  0.745917\n",
       "4     0.700000   0.636364  0.777778  0.802921\n",
       "...        ...        ...       ...       ...\n",
       "8167  0.055556   0.055556  0.055556  0.661060\n",
       "8168  0.473684   0.409091  0.562500  0.738859\n",
       "8169  0.417910   0.341463  0.538462  0.800686\n",
       "8170  0.166667   0.142857  0.200000  0.449487\n",
       "8171  0.588235   0.517241  0.681818  0.653666\n",
       "\n",
       "[8172 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rougedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411c61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a7187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6477d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
