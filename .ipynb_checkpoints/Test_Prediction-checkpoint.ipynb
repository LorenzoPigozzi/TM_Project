{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Project\n",
    "\n",
    "### NOVA IMS MT Metrics Shared Task\n",
    "\n",
    "**Group members:**\n",
    "- Lorenzo Pigozzi\n",
    "- Davide Farinati\n",
    "- Antonio\n",
    "- Luis Reis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "The goal of this project is to develop a metric that predicts the quality of a translation using the reference. \n",
    "\n",
    "\\\n",
    "**Evaluation**\\\n",
    "Your metric should correlate well with the existing quality assessments that you have in the above corpus.\n",
    "The one that we have so far are just the training sets, we will have a test set without the z-score, average score and annotators.\n",
    "Produce our own metric that will be compared with the existing ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents**\n",
    "\n",
    "1.\t[Importing libraries and corpora](#1)   \n",
    "2.\t[Brief Exploratory data analysis](#2)       \n",
    "3.\t[Cleaning](#3.)  \n",
    "4.\t[Predictions](#4.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries and corpora <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# word's preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the corpora\n",
    "cs_en = pd.read_csv('testset_TM\\cs-en\\scores.csv')\n",
    "de_en = pd.read_csv('testset_TM\\de-en\\scores.csv')\n",
    "ru_en = pd.read_csv('testset_TM\\ru-en\\scores.csv')\n",
    "zh_en = pd.read_csv('testset_TM\\zh-en\\scores.csv')\n",
    "en_fi = pd.read_csv('testset_TM\\en-fi\\scores.csv')\n",
    "en_zh = pd.read_csv('testset_TM\\en-zh\\scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Brief Exploratory Data Analysis <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cs_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28404"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25352"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zh_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8097"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One local resident who did not wish to be name...</td>\n",
       "      <td>Eräs paikallinen asukas, joka ei halunnut nime...</td>\n",
       "      <td>Toisen nimettömänä pysyttelevän asukkaan mukaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Still, she clings to a chant she's committed t...</td>\n",
       "      <td>Silti hän takertuu chant hän on sitoutunut mui...</td>\n",
       "      <td>Silti hän luottaa edelleen iskulauseeseen, jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't want to be asked, 'What were you doing...</td>\n",
       "      <td>En halua, että minulta kysytään: \"Mitä te teit...</td>\n",
       "      <td>En halua, että kenenkään tarvitsee kysyä minul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"I wouldn't say it was a lie – that's a pretty...</td>\n",
       "      <td>\"En sanoisi, että se oli valhe - se on aika ro...</td>\n",
       "      <td>En sanoisi, että se oli valhe, se on aika kova...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kari Kola took part in the opening ceremony of...</td>\n",
       "      <td>Kari Kola osallistui valon vuoden avajaisiin v...</td>\n",
       "      <td>Kari Kola oli mukana Valon teemavuoden avajais...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  One local resident who did not wish to be name...   \n",
       "1  Still, she clings to a chant she's committed t...   \n",
       "2  I don't want to be asked, 'What were you doing...   \n",
       "3  \"I wouldn't say it was a lie – that's a pretty...   \n",
       "4  Kari Kola took part in the opening ceremony of...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Eräs paikallinen asukas, joka ei halunnut nime...   \n",
       "1  Silti hän takertuu chant hän on sitoutunut mui...   \n",
       "2  En halua, että minulta kysytään: \"Mitä te teit...   \n",
       "3  \"En sanoisi, että se oli valhe - se on aika ro...   \n",
       "4  Kari Kola osallistui valon vuoden avajaisiin v...   \n",
       "\n",
       "                                         translation  \n",
       "0  Toisen nimettömänä pysyttelevän asukkaan mukaa...  \n",
       "1  Silti hän luottaa edelleen iskulauseeseen, jon...  \n",
       "2  En halua, että kenenkään tarvitsee kysyä minul...  \n",
       "3  En sanoisi, että se oli valhe, se on aika kova...  \n",
       "4  Kari Kola oli mukana Valon teemavuoden avajais...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of translations : ',len(cs_en) + len(de_en) + len(ru_en) + len(zh_en) + len(en_fi) + len(en_zh) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cleaning <a class=\"anchor\" id=\"3.\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          lower = False,\n",
    "          keep_numbers = False,\n",
    "          keep_expression = False,\n",
    "          remove_char = False,\n",
    "          remove_stop = False,\n",
    "          remove_tag = False,\n",
    "          lemmatize = False,\n",
    "          stemmer = False,\n",
    "          english = True\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    if english:\n",
    "        lang = 'english'\n",
    "    else:\n",
    "        lang = 'finnish'\n",
    "    \n",
    "    stop = set(stopwords.words(lang))\n",
    "    stem = SnowballStemmer(lang)\n",
    "    \n",
    "    updates = []\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "            \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if not keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        #KEEP '?' and '!' AS TOKENS\n",
    "        if not keep_expression:\n",
    "            text = re.sub(\"[\\?|\\!]\", 'EXPRESSION', text)\n",
    "            \n",
    "        #REMOVE TAGS\n",
    "        if remove_tag:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "            \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if remove_char:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            if english:\n",
    "                lemma = WordNetLemmatizer()\n",
    "                text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(stem.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def clean_ch(text_list, keep_numbers=False, remove_punctuation=False, remove_stop = False, stopwords_set='merged'):\n",
    "    \"\"\"\n",
    "    Function that removes chinese stopwords\n",
    "    \n",
    "    :param stopwords_set: remove words of both sets (merged), just the 1st (fst) or just the second (snd) \n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    \n",
    "    zh_stopwords1 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords1.txt', 'r', encoding='utf-8').readlines()]\n",
    "    zh_stopwords2 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords2.txt', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    if stopwords_set == 'merged':\n",
    "        stop = list(set(zh_stopwords1 + zh_stopwords2))\n",
    "    elif stopwords_set == 'fst':\n",
    "        stop = zh_stopwords1\n",
    "    elif stopwords_set == 'snd':\n",
    "        stop = zh_stopwords2\n",
    "\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        # REMOVE PUNCTUATION\n",
    "        if remove_punctuation:\n",
    "            # https://stackoverflow.com/questions/36640587/how-to-remove-chinese-punctuation-in-python\n",
    "            punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "            text = re.sub(r\"[%s]+\" %punc, \"\", text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            pretext = text\n",
    "            text = ' '.join([word for word in jieba.cut(text) if word not in stop])\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def update_df(dataframe, list_updated):\n",
    "    dataframe.update(pd.DataFrame({\"Text\": list_updated}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictions <a class=\"anchor\" id=\"4.\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
