{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3c2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee48cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c95e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_train_true, y_train_pred, y_dev_true, y_dev_pred, y_test_true, y_test_pred, return_corr = False):\n",
    "    \n",
    "    cleaned_corr_train, cleaned_corr_train_pvalue = pearsonr(y_train_true, y_train_pred)\n",
    "    cleaned_corr_ktau_train, cleaned_corr_ktau_train_pvalue = kendalltau(y_train_true, y_train_pred)\n",
    "    \n",
    "    cleaned_corr_dev, cleaned_corr_dev_pvalue = pearsonr(y_dev_true, y_dev_pred)\n",
    "    cleaned_corr_ktau_dev, cleaned_corr_ktau_dev_pvalue = kendalltau(y_dev_true, y_dev_pred)\n",
    "\n",
    "\n",
    "    cleaned_corr_ktau_test, cleaned_corr_ktau_test_pvalue = kendalltau(y_test_true, y_test_pred)\n",
    "    cleaned_corr_test, cleaned_corr_test_pvalue = pearsonr(y_test_true, y_test_pred)\n",
    "        \n",
    "    print(f'Train - Pearson: {cleaned_corr_train}; Kendall Tau: {cleaned_corr_ktau_train}')\n",
    "    print(f'Dev - Pearson: {cleaned_corr_dev}; Kendall Tau: {cleaned_corr_ktau_dev}')\n",
    "    print(f'Test - Pearson: {cleaned_corr_test}; Kendall Tau: {cleaned_corr_ktau_test}')\n",
    "    \n",
    "    if return_corr:\n",
    "        return cleaned_corr_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce514b72",
   "metadata": {
    "id": "jA_9c3SpLTUn"
   },
   "source": [
    "# Data Importing and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb58d83f",
   "metadata": {
    "id": "nU_gOCWtF8XY"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('corpus')\n",
    "files.remove('.DS_Store')\n",
    "files.remove('scores_ru-en.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for file_ in files:\n",
    "  name = file_.split('-')[0] + file_.split('-')[1]\n",
    "  vars()[name] = pd.read_csv(os.path.join('corpus', file_, 'scores.csv'))\n",
    "  vars()[name].drop(columns = ['source', 'annotators', 'avg-score'], inplace = True)\n",
    "  vars()[name]['z-score'] = scaler.fit_transform(vars()[name]['z-score'].values.reshape(-1,1)) #normalizing values betwewen 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1ef1c2",
   "metadata": {
    "id": "EcUEx6elHvwd"
   },
   "outputs": [],
   "source": [
    "english = csen.copy()\n",
    "for df in [deen, ruen, zhen]:\n",
    "  english = english.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074be1cf",
   "metadata": {
    "id": "8ziEKCv4LFwE"
   },
   "outputs": [],
   "source": [
    "finnish = enfi.copy()\n",
    "chinese = enzh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddc2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "english.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197ad40",
   "metadata": {},
   "source": [
    "# Clean corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0248aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          lower = False,\n",
    "          keep_numbers = False,\n",
    "          keep_expression = False,\n",
    "          remove_char = False,\n",
    "          remove_stop = False,\n",
    "          remove_tag = False,\n",
    "          lemmatize = False,\n",
    "          stemmer = False,\n",
    "          english = True\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    if english:\n",
    "        lang = 'english'\n",
    "    else:\n",
    "        lang = 'finnish'\n",
    "    \n",
    "    stop = set(stopwords.words(lang))\n",
    "    stem = SnowballStemmer(lang)\n",
    "    \n",
    "    updates = []\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "            \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if not keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        #KEEP '?' and '!' AS TOKENS\n",
    "        if not keep_expression:\n",
    "            text = re.sub(\"[\\?|\\!]\", 'EXPRESSION', text)\n",
    "            \n",
    "        #REMOVE TAGS\n",
    "        if remove_tag:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "            \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if remove_char:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            if english:\n",
    "                lemma = WordNetLemmatizer()\n",
    "                text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(stem.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def clean_ch(text_list, keep_numbers=False, remove_punctuation=False, remove_stop = False, stopwords_set='merged'):\n",
    "    \"\"\"\n",
    "    Function that removes chinese stopwords\n",
    "    \n",
    "    :param stopwords_set: remove words of both sets (merged), just the 1st (fst) or just the second (snd) \n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    \n",
    "    zh_stopwords1 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords1.txt', 'r', encoding='utf-8').readlines()]\n",
    "    zh_stopwords2 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords2.txt', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    if stopwords_set == 'merged':\n",
    "        stop = list(set(zh_stopwords1 + zh_stopwords2))\n",
    "    elif stopwords_set == 'fst':\n",
    "        stop = zh_stopwords1\n",
    "    elif stopwords_set == 'snd':\n",
    "        stop = zh_stopwords2\n",
    "\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        # REMOVE PUNCTUATION\n",
    "        if remove_punctuation:\n",
    "            # https://stackoverflow.com/questions/36640587/how-to-remove-chinese-punctuation-in-python\n",
    "            punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "            text = re.sub(r\"[%s]+\" %punc, \"\", text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            pretext = text\n",
    "            text = ' '.join([word for word in jieba.cut(text) if word not in stop])\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8156998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\anton\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.547 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# En\n",
    "cleaning_dict = {'lower': False, 'keep_numbers': True, 'keep_expression': False, 'remove_char': True, 'remove_stop': True, 'remove_tag': False, 'lemmatize': False, 'stemmer': True}\n",
    "english_clean = pd.DataFrame()\n",
    "english_clean['z-score'] = english['z-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    english_clean[column] = clean(english[column], cleaning_dict)\n",
    "    \n",
    "# Fi\n",
    "finnish_clean = pd.DataFrame()\n",
    "for c in ['reference', 'translation']:\n",
    "    finnish_clean[c] = clean(finnish[c],\n",
    "                             lower = False,\n",
    "                             keep_numbers = False,\n",
    "                             keep_expression = True,\n",
    "                             remove_char = True,\n",
    "                             remove_stop = False,\n",
    "                             remove_tag = True,\n",
    "                             lemmatize = False,\n",
    "                             stemmer = True,\n",
    "                             english=False)\n",
    "finnish_clean['z-score'] = finnish['z-score']\n",
    "\n",
    "#Ch\n",
    "chinese_clean = pd.DataFrame()\n",
    "for c in ['reference', 'translation']:\n",
    "    chinese_clean[c] = clean_ch(chinese[c],\n",
    "                                keep_numbers = False,\n",
    "                                remove_punctuation = False,\n",
    "                                remove_stop = True,\n",
    "                                stopwords_set = 'snd')\n",
    "chinese_clean['z-score'] = chinese['z-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e63d2",
   "metadata": {
    "id": "NjBupUyDLNRa"
   },
   "source": [
    "# Train, Dev & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5d3aa6",
   "metadata": {
    "id": "qZKkQkBNLSYT"
   },
   "outputs": [],
   "source": [
    "# English\n",
    "en_train, en_dev = train_test_split(english, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev, en_test = train_test_split(en_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "en_train_clean, en_dev_clean = train_test_split(english_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "en_dev_clean, en_test_clean = train_test_split(en_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "\n",
    "# Finnish\n",
    "fin_train, fin_dev = train_test_split(finnish, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "fin_dev, fin_test = train_test_split(fin_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "fin_train_clean, fin_dev_clean = train_test_split(finnish_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "fin_dev_clean, fin_test_clean = train_test_split(fin_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "\n",
    "#Chinese\n",
    "ch_train, ch_dev = train_test_split(chinese, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "ch_dev, ch_test = train_test_split(ch_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "ch_train_clean, ch_dev_clean = train_test_split(chinese_clean, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "ch_dev_clean, ch_test_clean = train_test_split(ch_dev_clean, shuffle = True, test_size = 0.5, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db87e2",
   "metadata": {},
   "source": [
    "# BLEU (original corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af42813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(dfCorpus, gram, chinese = False):\n",
    "    if gram == 1:\n",
    "        if chinese:\n",
    "            dfCorpus[str(gram)+'-gram'] = dfCorpus.apply(lambda x: sentence_bleu([list(jieba.cut(x.reference))], list(jieba.cut(x.translation)), \n",
    "                                                                                 weights=(1, 0, 0, 0)), axis=1)\n",
    "        else:\n",
    "            dfCorpus[str(gram)+'-gram'] = dfCorpus.apply(lambda x: sentence_bleu([x.reference.split(' ')], x.translation.split(' '), \n",
    "                                                                                 weights=(1, 0, 0, 0)), axis=1)\n",
    "    elif gram == 2:\n",
    "        if chinese:\n",
    "            dfCorpus[str(gram)+'-gram'] = dfCorpus.apply(lambda x: sentence_bleu([list(jieba.cut(x.reference))], list(jieba.cut(x.translation)), \n",
    "                                                                                 weights=(0.5, 0.5, 0, 0)), axis=1)\n",
    "        else:\n",
    "            dfCorpus[str(gram)+'-gram'] = dfCorpus.apply(lambda x: sentence_bleu([x.reference.split(' ')], x.translation.split(' '), \n",
    "                                                                                 weights=(0.5, 0.5, 0, 0)), axis=1)\n",
    "    elif gram == 3:\n",
    "        if chinese:\n",
    "            dfCorpus[str(gram)+'-gram'] = dfCorpus.apply(lambda x: sentence_bleu([list(jieba.cut(x.reference))], list(jieba.cut(x.translation)), \n",
    "                                                                             weights=(0.33, 0.33, 0.33, 0)), axis=1)\n",
    "        else:\n",
    "            dfCorpus[str(gram)+'-gram'] = dfCorpus.apply(lambda x: sentence_bleu([x.reference.split(' ')], x.translation.split(' '), \n",
    "                                                                             weights=(0.33, 0.33, 0.33, 0)), axis=1)\n",
    "    else:\n",
    "        if chinese:\n",
    "            dfCorpus['4-gram'] = dfCorpus.apply(lambda x: sentence_bleu([list(jieba.cut(x.reference))], list(jieba.cut(x.translation))), axis=1)\n",
    "        else:\n",
    "            dfCorpus['4-gram'] = dfCorpus.apply(lambda x: sentence_bleu([x.reference.split(' ')], x.translation.split(' ')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3148404c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_train\n",
      "en_dev\n",
      "en_test\n",
      "fin_train\n",
      "fin_dev\n",
      "fin_test\n",
      "ch_train\n",
      "ch_dev\n",
      "ch_test\n"
     ]
    }
   ],
   "source": [
    "raw_sets = [en_train, en_dev, en_test, fin_train, fin_dev, fin_test, ch_train, ch_dev, ch_test]\n",
    "raw_names = ['en_train', 'en_dev', 'en_test', 'fin_train', 'fin_dev', 'fin_test', 'ch_train', 'ch_dev', 'ch_test']\n",
    "for l in range(len(raw_sets)):\n",
    "    print(raw_names[l])\n",
    "    if l < 6:\n",
    "        for i in range(1, 5):\n",
    "            bleu_score(raw_sets[l], i)\n",
    "    else:\n",
    "        for i in range(1, 5):\n",
    "            bleu_score(raw_sets[l], i, chinese = True)\n",
    "    raw_sets[l].to_csv('bleu_scores/' + raw_names[l] +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00ae235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "1-gram\n",
      "Train - Pearson: 0.30121724967561003; Kendall Tau: 0.20086241876059335\n",
      "Dev - Pearson: 0.2897086783608815; Kendall Tau: 0.193424676433337\n",
      "Test - Pearson: 0.2991839411614783; Kendall Tau: 0.1987102507835839\n",
      "2-gram\n",
      "Train - Pearson: 0.2900661034718919; Kendall Tau: 0.19573777734908368\n",
      "Dev - Pearson: 0.28327001847085365; Kendall Tau: 0.18999390846583608\n",
      "Test - Pearson: 0.2906430222200428; Kendall Tau: 0.19419998979298964\n",
      "3-gram\n",
      "Train - Pearson: 0.2708019765581924; Kendall Tau: 0.18538307321371858\n",
      "Dev - Pearson: 0.26847056104903816; Kendall Tau: 0.1834512044767098\n",
      "Test - Pearson: 0.2761214358252421; Kendall Tau: 0.18772425938832216\n",
      "4-gram\n",
      "Train - Pearson: 0.23973283198574769; Kendall Tau: 0.17409073428145383\n",
      "Dev - Pearson: 0.2369017079276193; Kendall Tau: 0.17358385189391115\n",
      "Test - Pearson: 0.24686795947856094; Kendall Tau: 0.17890916422691888\n",
      "\n",
      "Finnish\n",
      "1-gram\n",
      "Train - Pearson: 0.5094910352193146; Kendall Tau: 0.3359875667818137\n",
      "Dev - Pearson: 0.5279442634028998; Kendall Tau: 0.3647709112970282\n",
      "Test - Pearson: 0.47402684760044844; Kendall Tau: 0.30189385075027547\n",
      "2-gram\n",
      "Train - Pearson: 0.4407360379975814; Kendall Tau: 0.31904198869456946\n",
      "Dev - Pearson: 0.4474093066517058; Kendall Tau: 0.33063345791475723\n",
      "Test - Pearson: 0.4058312190287918; Kendall Tau: 0.2864662051904055\n",
      "3-gram\n",
      "Train - Pearson: 0.3538648090812974; Kendall Tau: 0.30273696144559104\n",
      "Dev - Pearson: 0.3191672306253; Kendall Tau: 0.2973639542333361\n",
      "Test - Pearson: 0.3183041461978536; Kendall Tau: 0.265675774665962\n",
      "4-gram\n",
      "Train - Pearson: 0.2655723001082595; Kendall Tau: 0.29377765733424566\n",
      "Dev - Pearson: 0.2562845062255542; Kendall Tau: 0.2877844144454559\n",
      "Test - Pearson: 0.24091967658106633; Kendall Tau: 0.2594056222403194\n",
      "\n",
      "Chinese\n",
      "1-gram\n",
      "Train - Pearson: 0.4122207145905572; Kendall Tau: 0.2777718384304173\n",
      "Dev - Pearson: 0.4529144415529226; Kendall Tau: 0.3180541394141151\n",
      "Test - Pearson: 0.43776457909502825; Kendall Tau: 0.3090802291161836\n",
      "2-gram\n",
      "Train - Pearson: 0.42018899240326296; Kendall Tau: 0.2884205970124815\n",
      "Dev - Pearson: 0.4574571022351525; Kendall Tau: 0.321843091985438\n",
      "Test - Pearson: 0.4340610079007713; Kendall Tau: 0.3108742250767125\n",
      "3-gram\n",
      "Train - Pearson: 0.36332741185196277; Kendall Tau: 0.26932818670332465\n",
      "Dev - Pearson: 0.40364767624612496; Kendall Tau: 0.29595741670737236\n",
      "Test - Pearson: 0.3753189123210253; Kendall Tau: 0.28621668971160635\n",
      "4-gram\n",
      "Train - Pearson: 0.30333371199988574; Kendall Tau: 0.25503554180399246\n",
      "Dev - Pearson: 0.33807843691974865; Kendall Tau: 0.2795114928403453\n",
      "Test - Pearson: 0.3173738174299411; Kendall Tau: 0.27480805652814255\n"
     ]
    }
   ],
   "source": [
    "print('English')\n",
    "for i in range (1, 5):\n",
    "    print(str(i) + '-gram')\n",
    "    corr(en_train['z-score'], en_train[str(i) + '-gram'], en_dev['z-score'], en_dev[str(i) + '-gram'], en_test['z-score'], en_test[str(i) + '-gram'])\n",
    "print()\n",
    "print('Finnish')\n",
    "for i in range (1, 5):\n",
    "    print(str(i) + '-gram')\n",
    "    corr(fin_train['z-score'], fin_train[str(i) + '-gram'], fin_dev['z-score'], fin_dev[str(i) + '-gram'], fin_test['z-score'], fin_test[str(i) + '-gram'])\n",
    "print()\n",
    "print('Chinese')\n",
    "for i in range (1, 5):\n",
    "    print(str(i) + '-gram')\n",
    "    corr(ch_train['z-score'], ch_train[str(i) + '-gram'], ch_dev['z-score'], ch_dev[str(i) + '-gram'], ch_test['z-score'], ch_test[str(i) + '-gram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf0bcd",
   "metadata": {},
   "source": [
    "# BLEU (cleaned corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7491c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_train_clean\n",
      "en_dev_clean\n",
      "en_test_clean\n",
      "fin_train_clean\n",
      "fin_dev_clean\n",
      "fin_test_clean\n",
      "ch_train_clean\n",
      "ch_dev_clean\n",
      "ch_test_clean\n"
     ]
    }
   ],
   "source": [
    "raw_sets = [en_train_clean, en_dev_clean, en_test_clean, fin_train_clean, fin_dev_clean, fin_test_clean, ch_train_clean, ch_dev_clean, ch_test_clean]\n",
    "raw_names = ['en_train_clean', 'en_dev_clean', 'en_test_clean', 'fin_train_clean', 'fin_dev_clean', 'fin_test_clean', 'ch_train_clean', 'ch_dev_clean', 'ch_test_clean']\n",
    "for l in range(len(raw_sets)):\n",
    "    print(raw_names[l])\n",
    "    if l < 6:\n",
    "        for i in range(1, 5):\n",
    "            bleu_score(raw_sets[l], i)\n",
    "    else:\n",
    "        for i in range(1, 5):\n",
    "            bleu_score(raw_sets[l], i, chinese = True)\n",
    "    raw_sets[l].to_csv('bleu_scores/' + raw_names[l] +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3254abb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "1-gram\n",
      "Train - Pearson: 0.3007898882711333; Kendall Tau: 0.20017404288830293\n",
      "Dev - Pearson: 0.28930084544123347; Kendall Tau: 0.1932067428574309\n",
      "Test - Pearson: 0.3047431153999639; Kendall Tau: 0.2027637219263774\n",
      "2-gram\n",
      "Train - Pearson: 0.28957145528016764; Kendall Tau: 0.19469184567342454\n",
      "Dev - Pearson: 0.27753556277032243; Kendall Tau: 0.18688073186152832\n",
      "Test - Pearson: 0.29278691402405405; Kendall Tau: 0.19706146003275488\n",
      "3-gram\n",
      "Train - Pearson: 0.2702715166174466; Kendall Tau: 0.18407877893489033\n",
      "Dev - Pearson: 0.2656186223576372; Kendall Tau: 0.18074664125093398\n",
      "Test - Pearson: 0.2813576461849093; Kendall Tau: 0.1902968517911969\n",
      "4-gram\n",
      "Train - Pearson: 0.2411480682463999; Kendall Tau: 0.17305414780020778\n",
      "Dev - Pearson: 0.23530460307654394; Kendall Tau: 0.17054567083557362\n",
      "Test - Pearson: 0.25006928279591495; Kendall Tau: 0.18046011334247844\n",
      "\n",
      "Finnish\n",
      "1-gram\n",
      "Train - Pearson: 0.5570731091505411; Kendall Tau: 0.35799648628736325\n",
      "Dev - Pearson: 0.587084481179909; Kendall Tau: 0.39770838157376626\n",
      "Test - Pearson: 0.5041167003375; Kendall Tau: 0.3198604443062156\n",
      "2-gram\n",
      "Train - Pearson: 0.5018023875615458; Kendall Tau: 0.3423149115460669\n",
      "Dev - Pearson: 0.5326360395816246; Kendall Tau: 0.3687271466495364\n",
      "Test - Pearson: 0.44876921131357994; Kendall Tau: 0.2981628911872342\n",
      "3-gram\n",
      "Train - Pearson: 0.41057213278130916; Kendall Tau: 0.3134543214459546\n",
      "Dev - Pearson: 0.44340155500862977; Kendall Tau: 0.33213272711924596\n",
      "Test - Pearson: 0.36073661228637877; Kendall Tau: 0.26770591049799575\n",
      "4-gram\n",
      "Train - Pearson: 0.3408989922402146; Kendall Tau: 0.29744615818892384\n",
      "Dev - Pearson: 0.3499582761047969; Kendall Tau: 0.3065058046329946\n",
      "Test - Pearson: 0.31824941409320867; Kendall Tau: 0.25342873270458605\n",
      "\n",
      "Chinese\n",
      "1-gram\n",
      "Train - Pearson: 0.33217413169295795; Kendall Tau: 0.22575966321635843\n",
      "Dev - Pearson: 0.36170859452774984; Kendall Tau: 0.25485738434809707\n",
      "Test - Pearson: 0.3566673215925983; Kendall Tau: 0.2616582530664564\n",
      "2-gram\n",
      "Train - Pearson: 0.3967469111835905; Kendall Tau: 0.26056094873442714\n",
      "Dev - Pearson: 0.42355260621237745; Kendall Tau: 0.28978386091056013\n",
      "Test - Pearson: 0.42505278823898784; Kendall Tau: 0.29415145681967814\n",
      "3-gram\n",
      "Train - Pearson: 0.39295249688330397; Kendall Tau: 0.26573081883137645\n",
      "Dev - Pearson: 0.4076838858892239; Kendall Tau: 0.2855644340158522\n",
      "Test - Pearson: 0.4200273281847648; Kendall Tau: 0.29617084696675205\n",
      "4-gram\n",
      "Train - Pearson: 0.37453745038991326; Kendall Tau: 0.26413445785368866\n",
      "Dev - Pearson: 0.3767725795613109; Kendall Tau: 0.27305194785255027\n",
      "Test - Pearson: 0.39179008110554203; Kendall Tau: 0.28751178008956546\n"
     ]
    }
   ],
   "source": [
    "print('English')\n",
    "for i in range (1, 5):\n",
    "    print(str(i) + '-gram')\n",
    "    corr(en_train_clean['z-score'], en_train_clean[str(i) + '-gram'], en_dev_clean['z-score'], en_dev_clean[str(i) + '-gram'], en_test_clean['z-score'], en_test_clean[str(i) + '-gram'])\n",
    "print()\n",
    "print('Finnish')\n",
    "for i in range (1, 5):\n",
    "    print(str(i) + '-gram')\n",
    "    corr(fin_train_clean['z-score'], fin_train_clean[str(i) + '-gram'], fin_dev_clean['z-score'], fin_dev_clean[str(i) + '-gram'], fin_test_clean['z-score'], fin_test_clean[str(i) + '-gram'])\n",
    "print()\n",
    "print('Chinese')\n",
    "for i in range (1, 5):\n",
    "    print(str(i) + '-gram')\n",
    "    corr(ch_train_clean['z-score'], ch_train_clean[str(i) + '-gram'], ch_dev_clean['z-score'], ch_dev_clean[str(i) + '-gram'], ch_test_clean['z-score'], ch_test_clean[str(i) + '-gram'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
