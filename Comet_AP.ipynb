{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1705b0f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'comet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-67a39c3ad902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkendalltau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlibvoikko\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcomet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'comet'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "import libvoikko\n",
    "import comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33430ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unbabel-comet\n",
      "  Using cached unbabel_comet-0.1.0-py3-none-any.whl (53 kB)\n",
      "Collecting fsspec==0.8.7\n",
      "  Using cached fsspec-0.8.7-py3-none-any.whl (103 kB)\n",
      "Collecting pytorch-lightning<=1.3\n",
      "  Using cached pytorch_lightning-1.3.0-py3-none-any.whl (804 kB)\n",
      "Collecting tensorboard==2.2.0\n",
      "  Using cached tensorboard-2.2.0-py3-none-any.whl (2.8 MB)\n",
      "Collecting transformers<5.0.0,>=4.0.0\n",
      "  Using cached transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
      "Requirement already satisfied: numpy<1.20.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from unbabel-comet) (1.19.2)\n",
      "Collecting fairseq==0.9.0\n",
      "  Using cached fairseq-0.9.0.tar.gz (306 kB)\n",
      "Collecting scipy<1.6.0,>=1.5.0\n",
      "  Using cached scipy-1.5.4-cp38-cp38-win_amd64.whl (31.4 MB)\n",
      "Collecting scikit-learn==0.24\n",
      "  Using cached scikit_learn-0.24.0-cp38-cp38-win_amd64.whl (6.9 MB)\n",
      "Collecting pytorch-nlp==0.5.0\n",
      "  Using cached pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
      "Collecting torch<=1.6\n",
      "  Using cached torch-0.1.2.post2.tar.gz (128 kB)\n",
      "Requirement already satisfied: PyYAML<5.4.0,>=5.3.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from unbabel-comet) (5.3.1)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Using cached sentencepiece-0.1.91-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from unbabel-comet) (1.2.4)\n",
      "Requirement already satisfied: cffi in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from fairseq==0.9.0->unbabel-comet) (1.14.5)\n",
      "Collecting cython\n",
      "  Using cached Cython-0.29.23-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: regex in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from fairseq==0.9.0->unbabel-comet) (2021.4.4)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from fairseq==0.9.0->unbabel-comet) (4.59.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from scikit-learn==0.24->unbabel-comet) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from scikit-learn==0.24->unbabel-comet) (2.1.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard==2.2.0->unbabel-comet) (2.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard==2.2.0->unbabel-comet) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard==2.2.0->unbabel-comet) (0.36.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Using cached grpcio-1.37.1-cp38-cp38-win_amd64.whl (3.1 MB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Using cached protobuf-3.17.0-py2.py3-none-any.whl (173 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard==2.2.0->unbabel-comet) (1.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard==2.2.0->unbabel-comet) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard==2.2.0->unbabel-comet) (1.30.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->unbabel-comet) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->unbabel-comet) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->unbabel-comet) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from pandas<2.0.0,>=1.0.0->unbabel-comet) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from pandas<2.0.0,>=1.0.0->unbabel-comet) (2.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.0->unbabel-comet) (0.4.8)\n",
      "Collecting fsspec[http]>=2021.4.0\n",
      "  Using cached fsspec-2021.4.0-py3-none-any.whl (108 kB)\n",
      "Collecting pytorch-lightning<=1.3\n",
      "  Using cached pytorch_lightning-1.2.10-py3-none-any.whl (841 kB)\n",
      "Collecting torchmetrics==0.2.0\n",
      "  Using cached torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
      "Collecting pytorch-lightning<=1.3\n",
      "  Using cached pytorch_lightning-1.2.9-py3-none-any.whl (841 kB)\n",
      "  Using cached pytorch_lightning-1.2.8-py3-none-any.whl (841 kB)\n",
      "  Using cached pytorch_lightning-1.2.7-py3-none-any.whl (830 kB)\n",
      "  Using cached pytorch_lightning-1.2.6-py3-none-any.whl (829 kB)\n",
      "  Using cached pytorch_lightning-1.2.5-py3-none-any.whl (826 kB)\n",
      "  Using cached pytorch_lightning-1.2.4-py3-none-any.whl (829 kB)\n",
      "  Using cached pytorch_lightning-1.2.3-py3-none-any.whl (821 kB)\n",
      "  Using cached pytorch_lightning-1.2.2-py3-none-any.whl (816 kB)\n",
      "  Using cached pytorch_lightning-1.2.1-py3-none-any.whl (814 kB)\n",
      "  Using cached pytorch_lightning-1.2.0-py3-none-any.whl (813 kB)\n",
      "  Using cached pytorch_lightning-1.1.8-py3-none-any.whl (696 kB)\n",
      "  Using cached pytorch_lightning-1.1.7-py3-none-any.whl (695 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\test-tube_36e8268cf9434152ab8f69b3b38c4e81\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\test-tube_36e8268cf9434152ab8f69b3b38c4e81\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\anton\\AppData\\Local\\Temp\\pip-pip-egg-info-e8u39t6d'\n",
      "         cwd: C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\test-tube_36e8268cf9434152ab8f69b3b38c4e81\\\n",
      "    Complete output (7 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\test-tube_36e8268cf9434152ab8f69b3b38c4e81\\setup.py\", line 28, in <module>\n",
      "        install_requires=load_requirements(PATH_ROOT),\n",
      "      File \"C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\test-tube_36e8268cf9434152ab8f69b3b38c4e81\\setup.py\", line 10, in load_requirements\n",
      "        with open(os.path.join(path_dir, 'requirements.txt'), 'r') as file:\n",
      "    FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\test-tube_36e8268cf9434152ab8f69b3b38c4e81\\\\requirements.txt'\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/dc/7a/38570026d3a58684348b7db8853c9c1a62f05e5ec6610716b8e92216e1bc/test_tube-0.7.4.tar.gz#sha256=0a827bd847eea6b7fbcd6bd80b2f7494fbe68d91d28280335dd26bd0c41e26ed (from https://pypi.org/simple/test-tube/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "WARNING: The candidate selected for download or install is a yanked version: 'torch' candidate (version 0.1.2.post2 at https://files.pythonhosted.org/packages/f8/02/880b468bd382dc79896eaecbeb8ce95e9c4b99a24902874a2cef0b562cea/torch-0.1.2.post2.tar.gz#sha256=a43e37f8f927c5b18f80cd163daaf6a1920edafcab5102e02e3e14bb97d9c874 (from https://pypi.org/simple/torch/))\n",
      "Reason for being yanked: 0.1.2 is past it's support date and confuses users on unsupported platforms\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\fairseq_9db01a32aac740deb5c06666c02d30b7\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\fairseq_9db01a32aac740deb5c06666c02d30b7\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\anton\\AppData\\Local\\Temp\\pip-wheel-vxwjqnrr'\n",
      "       cwd: C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\fairseq_9db01a32aac740deb5c06666c02d30b7\\\n",
      "  Complete output (268 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\examples\n",
      "  copying examples\\__init__.py -> build\\lib.win-amd64-3.8\\examples\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\binarizer.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\bleu.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\checkpoint_utils.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\distributed_utils.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\file_utils.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\hub_utils.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\iterative_refinement_generator.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\legacy_distributed_data_parallel.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\meters.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\options.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\pdb.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\progress_bar.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\registry.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\search.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\sequence_generator.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\sequence_scorer.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\tokenizer.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\trainer.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\utils.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  copying fairseq\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\eval_lm.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\generate.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\interactive.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\preprocess.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\score.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\setup.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\train.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  copying fairseq_cli\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq_cli\n",
      "  creating build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\rerank.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\rerank_generate.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\rerank_options.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\rerank_score_bw.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\rerank_score_lm.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\rerank_tune.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\rerank_utils.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  copying examples\\noisychannel\\__init__.py -> build\\lib.win-amd64-3.8\\examples\\noisychannel\n",
      "  creating build\\lib.win-amd64-3.8\\examples\\speech_recognition\n",
      "  copying examples\\speech_recognition\\infer.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\n",
      "  copying examples\\speech_recognition\\w2l_decoder.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\n",
      "  copying examples\\speech_recognition\\__init__.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\n",
      "  creating build\\lib.win-amd64-3.8\\examples\\speech_recognition\\criterions\n",
      "  copying examples\\speech_recognition\\criterions\\ASG_loss.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\criterions\n",
      "  copying examples\\speech_recognition\\criterions\\cross_entropy_acc.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\criterions\n",
      "  copying examples\\speech_recognition\\criterions\\CTC_loss.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\criterions\n",
      "  copying examples\\speech_recognition\\criterions\\__init__.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\criterions\n",
      "  creating build\\lib.win-amd64-3.8\\examples\\speech_recognition\\data\n",
      "  copying examples\\speech_recognition\\data\\asr_dataset.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\data\n",
      "  copying examples\\speech_recognition\\data\\collaters.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\data\n",
      "  copying examples\\speech_recognition\\data\\data_utils.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\data\n",
      "  copying examples\\speech_recognition\\data\\replabels.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\data\n",
      "  copying examples\\speech_recognition\\data\\__init__.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\data\n",
      "  creating build\\lib.win-amd64-3.8\\examples\\speech_recognition\\models\n",
      "  copying examples\\speech_recognition\\models\\vggtransformer.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\models\n",
      "  copying examples\\speech_recognition\\models\\w2l_conv_glu_enc.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\models\n",
      "  copying examples\\speech_recognition\\models\\__init__.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\models\n",
      "  creating build\\lib.win-amd64-3.8\\examples\\speech_recognition\\tasks\n",
      "  copying examples\\speech_recognition\\tasks\\speech_recognition.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\tasks\n",
      "  copying examples\\speech_recognition\\tasks\\__init__.py -> build\\lib.win-amd64-3.8\\examples\\speech_recognition\\tasks\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\adaptive_loss.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\binary_cross_entropy.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\composite_loss.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\cross_entropy.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\fairseq_criterion.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\label_smoothed_cross_entropy.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\label_smoothed_cross_entropy_with_alignment.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\legacy_masked_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\masked_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\nat_loss.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\sentence_prediction.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\sentence_ranking.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  copying fairseq\\criterions\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\criterions\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\append_token_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\backtranslation_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\base_wrapper_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\colorize_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\concat_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\concat_sentences_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\data_utils.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\denoising_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\dictionary.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\fairseq_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\id_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\indexed_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\iterators.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\language_pair_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\list_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\lm_context_window_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\lru_cache_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\mask_tokens_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\monolingual_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\multi_corpus_sampled_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\nested_dictionary_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\noising.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\numel_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\num_samples_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\offset_tokens_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\pad_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\plasma_utils.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\prepend_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\prepend_token_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\raw_label_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\replace_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\resampling_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\roll_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\round_robin_zip_datasets.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\sharded_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\sort_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\strip_token_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\subsample_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\token_block_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\transform_eos_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\transform_eos_lang_pair_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\truncate_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  copying fairseq\\data\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\data\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\cmlm_transformer.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\composite_encoder.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\distributed_fairseq_model.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\fairseq_decoder.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\fairseq_encoder.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\fairseq_incremental_decoder.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\fairseq_model.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\fconv.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\fconv_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\fconv_self_att.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\insertion_transformer.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\iterative_nonautoregressive_transformer.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\levenshtein_transformer.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\lightconv.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\lightconv_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\lstm.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\masked_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\model_utils.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\multilingual_transformer.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\nonautoregressive_ensembles.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\nonautoregressive_transformer.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\transformer.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\transformer_from_pretrained_xlm.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\transformer_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\wav2vec.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  copying fairseq\\models\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\models\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\adaptive_input.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\adaptive_softmax.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\beamable_mm.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\character_token_embedder.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\conv_tbc.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\downsampled_multihead_attention.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\dynamic_convolution.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\gelu.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\grad_multiply.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\highway.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\layer_norm.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\learned_positional_embedding.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\lightweight_convolution.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\linearized_convolution.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\logsumexp_moe.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\mean_pool_gating_network.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\multihead_attention.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\positional_embedding.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\scalar_bias.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\sinusoidal_positional_embedding.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\sparse_multihead_attention.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\sparse_transformer_sentence_encoder.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\sparse_transformer_sentence_encoder_layer.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\transformer_layer.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\transformer_sentence_encoder.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\transformer_sentence_encoder_layer.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\unfold.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\vggblock.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  copying fairseq\\modules\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\adadelta.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\adafactor.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\adagrad.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\adam.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\adamax.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\bmuf.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\fairseq_optimizer.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\fp16_optimizer.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\nag.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\sgd.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  copying fairseq\\optim\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\audio_pretraining.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\cross_lingual_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\denoising.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\fairseq_task.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\language_modeling.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\legacy_masked_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\masked_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached pytorch_lightning-1.1.6-py3-none-any.whl (687 kB)\n",
      "  Using cached pytorch_lightning-1.1.5-py3-none-any.whl (685 kB)\n",
      "  Using cached pytorch_lightning-1.1.4-py3-none-any.whl (684 kB)\n",
      "  Using cached pytorch_lightning-1.1.3-py3-none-any.whl (680 kB)\n",
      "  Using cached pytorch_lightning-1.1.2-py3-none-any.whl (671 kB)\n",
      "  Using cached pytorch_lightning-1.1.1-py3-none-any.whl (669 kB)\n",
      "  Using cached pytorch_lightning-1.1.0-py3-none-any.whl (665 kB)\n",
      "  Using cached pytorch_lightning-1.0.8-py3-none-any.whl (561 kB)\n",
      "  Using cached pytorch_lightning-1.0.7-py3-none-any.whl (557 kB)\n",
      "  Using cached pytorch_lightning-1.0.6-py3-none-any.whl (548 kB)\n",
      "  Using cached pytorch_lightning-1.0.5-py3-none-any.whl (559 kB)\n",
      "  Using cached pytorch_lightning-1.0.4-py3-none-any.whl (554 kB)\n",
      "  Using cached pytorch_lightning-1.0.3-py3-none-any.whl (533 kB)\n",
      "  Using cached pytorch_lightning-1.0.2-py3-none-any.whl (532 kB)\n",
      "  Using cached pytorch_lightning-1.0.1-py3-none-any.whl (511 kB)\n",
      "  Using cached pytorch_lightning-1.0.0-py3-none-any.whl (510 kB)\n",
      "  Using cached pytorch_lightning-0.10.0-py3-none-any.whl (471 kB)\n",
      "  Using cached pytorch_lightning-0.9.0-py3-none-any.whl (408 kB)\n",
      "  Using cached pytorch_lightning-0.8.5-py3-none-any.whl (313 kB)\n",
      "  Using cached pytorch_lightning-0.8.4-py3-none-any.whl (304 kB)\n",
      "  Using cached pytorch_lightning-0.8.3-py3-none-any.whl (302 kB)\n",
      "  Using cached pytorch_lightning-0.8.1-py3-none-any.whl (293 kB)\n",
      "  Using cached pytorch_lightning-0.7.6-py3-none-any.whl (248 kB)\n",
      "  Using cached pytorch_lightning-0.7.5-py3-none-any.whl (233 kB)\n",
      "  Using cached pytorch_lightning-0.7.3-py3-none-any.whl (203 kB)\n",
      "  Using cached pytorch-lightning-0.7.1.tar.gz (6.0 MB)\n",
      "  Using cached pytorch-lightning-0.6.0.tar.gz (95 kB)\n",
      "  Using cached pytorch-lightning-0.5.3.3.tar.gz (95 kB)\n",
      "  Using cached pytorch-lightning-0.5.3.2.tar.gz (65 kB)\n",
      "  Using cached pytorch-lightning-0.5.3.1.tar.gz (55 kB)\n",
      "  Using cached pytorch-lightning-0.5.3.tar.gz (55 kB)\n",
      "  Using cached pytorch-lightning-0.5.2.1.tar.gz (56 kB)\n",
      "  Using cached pytorch-lightning-0.5.2.tar.gz (56 kB)\n",
      "  Using cached pytorch-lightning-0.5.1.3.tar.gz (55 kB)\n",
      "  Using cached pytorch-lightning-0.5.1.2.tar.gz (55 kB)\n",
      "  Using cached pytorch-lightning-0.5.1.tar.gz (55 kB)\n",
      "  Using cached pytorch-lightning-0.5.0.tar.gz (55 kB)\n",
      "  Using cached pytorch-lightning-0.4.9.tar.gz (55 kB)\n",
      "  Using cached pytorch-lightning-0.4.8.tar.gz (50 kB)\n",
      "  Using cached pytorch-lightning-0.4.7.tar.gz (51 kB)\n",
      "  Using cached pytorch-lightning-0.4.6.tar.gz (50 kB)\n",
      "  Using cached pytorch-lightning-0.4.5.tar.gz (47 kB)\n",
      "  Using cached pytorch-lightning-0.4.4.tar.gz (47 kB)\n",
      "  Using cached pytorch-lightning-0.4.3.tar.gz (44 kB)\n",
      "  Using cached pytorch-lightning-0.4.2.tar.gz (45 kB)\n",
      "  Using cached pytorch-lightning-0.4.1.tar.gz (45 kB)\n",
      "  Using cached pytorch-lightning-0.4.0.tar.gz (44 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.9.tar.gz (369 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.8.tar.gz (369 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.7.tar.gz (369 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.6.tar.gz (369 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.5.tar.gz (369 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  copying fairseq\\tasks\\multilingual_masked_lm.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\multilingual_translation.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\semisupervised_translation.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\sentence_prediction.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\sentence_ranking.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\translation.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\translation_from_pretrained_xlm.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\translation_lev.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\translation_moe.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  copying fairseq\\tasks\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\tasks\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\data\\audio\n",
      "  copying fairseq\\data\\audio\\raw_audio_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\audio\n",
      "  copying fairseq\\data\\audio\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\audio\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\fastbpe.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\gpt2_bpe.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\gpt2_bpe_utils.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\hf_bert_bpe.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\moses_tokenizer.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\nltk_tokenizer.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\sentencepiece_bpe.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\space_tokenizer.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\subword_nmt_bpe.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\utils.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  copying fairseq\\data\\encoders\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\encoders\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\data\\legacy\n",
      "  copying fairseq\\data\\legacy\\block_pair_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\legacy\n",
      "  copying fairseq\\data\\legacy\\masked_lm_dataset.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\legacy\n",
      "  copying fairseq\\data\\legacy\\masked_lm_dictionary.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\legacy\n",
      "  copying fairseq\\data\\legacy\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\data\\legacy\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\models\\bart\n",
      "  copying fairseq\\models\\bart\\hub_interface.py -> build\\lib.win-amd64-3.8\\fairseq\\models\\bart\n",
      "  copying fairseq\\models\\bart\\model.py -> build\\lib.win-amd64-3.8\\fairseq\\models\\bart\n",
      "  copying fairseq\\models\\bart\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\models\\bart\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\models\\roberta\n",
      "  copying fairseq\\models\\roberta\\alignment_utils.py -> build\\lib.win-amd64-3.8\\fairseq\\models\\roberta\n",
      "  copying fairseq\\models\\roberta\\hub_interface.py -> build\\lib.win-amd64-3.8\\fairseq\\models\\roberta\n",
      "  copying fairseq\\models\\roberta\\model.py -> build\\lib.win-amd64-3.8\\fairseq\\models\\roberta\n",
      "  copying fairseq\\models\\roberta\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\models\\roberta\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\modules\\dynamicconv_layer\n",
      "  copying fairseq\\modules\\dynamicconv_layer\\cuda_function_gen.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\dynamicconv_layer\n",
      "  copying fairseq\\modules\\dynamicconv_layer\\dynamicconv_layer.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\dynamicconv_layer\n",
      "  copying fairseq\\modules\\dynamicconv_layer\\setup.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\dynamicconv_layer\n",
      "  copying fairseq\\modules\\dynamicconv_layer\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\dynamicconv_layer\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\modules\\lightconv_layer\n",
      "  copying fairseq\\modules\\lightconv_layer\\cuda_function_gen.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\lightconv_layer\n",
      "  copying fairseq\\modules\\lightconv_layer\\lightconv_layer.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\lightconv_layer\n",
      "  copying fairseq\\modules\\lightconv_layer\\setup.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\lightconv_layer\n",
      "  copying fairseq\\modules\\lightconv_layer\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\modules\\lightconv_layer\n",
      "  creating build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\cosine_lr_scheduler.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\fairseq_lr_scheduler.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\fixed_schedule.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\inverse_square_root_schedule.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\polynomial_decay_schedule.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\reduce_lr_on_plateau.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\triangular_lr_scheduler.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\tri_stage_lr_scheduler.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  copying fairseq\\optim\\lr_scheduler\\__init__.py -> build\\lib.win-amd64-3.8\\fairseq\\optim\\lr_scheduler\n",
      "  running build_ext\n",
      "  cythoning fairseq/data/data_utils_fast.pyx to fairseq/data\\data_utils_fast.cpp\n",
      "  cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data\\token_block_utils_fast.cpp\n",
      "  warning: fairseq\\data\\token_block_utils_fast.pyx:99:40: the result of using negative indices inside of code sections marked as 'wraparound=False' is undefined\n",
      "  building 'fairseq.libbleu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for fairseq\n",
      "  ERROR: Command errored out with exit status 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached pytorch-lightning-0.3.6.4.tar.gz (369 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.3.tar.gz (369 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.1.tar.gz (370 kB)\n",
      "  Using cached pytorch-lightning-0.3.6.tar.gz (369 kB)\n",
      "  Using cached pytorch-lightning-0.3.5.tar.gz (368 kB)\n",
      "  Using cached pytorch-lightning-0.3.4.1.tar.gz (359 kB)\n",
      "  Using cached pytorch-lightning-0.3.4.tar.gz (359 kB)\n",
      "  Using cached pytorch-lightning-0.3.3.tar.gz (359 kB)\n",
      "  Using cached pytorch-lightning-0.3.2.tar.gz (359 kB)\n",
      "  Using cached pytorch-lightning-0.3.1.tar.gz (358 kB)\n",
      "  Using cached pytorch-lightning-0.3.tar.gz (357 kB)\n",
      "  Using cached pytorch-lightning-0.2.6.tar.gz (357 kB)\n",
      "  Using cached pytorch-lightning-0.2.5.2.tar.gz (357 kB)\n",
      "  Using cached pytorch-lightning-0.2.5.1.tar.gz (357 kB)\n",
      "  Using cached pytorch-lightning-0.2.5.tar.gz (357 kB)\n",
      "  Using cached pytorch-lightning-0.2.4.1.tar.gz (356 kB)\n",
      "  Using cached pytorch-lightning-0.2.4.tar.gz (356 kB)\n",
      "  Using cached pytorch-lightning-0.2.3.tar.gz (356 kB)\n",
      "  Using cached pytorch-lightning-0.2.2.tar.gz (356 kB)\n",
      "  Using cached pytorch-lightning-0.2.tar.gz (356 kB)\n",
      "  Using cached pytorch_lightning-0.0.2-py3-none-any.whl\n",
      "Collecting test-tube\n",
      "  Using cached test_tube-0.7.5.tar.gz (21 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->unbabel-comet) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->unbabel-comet) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->unbabel-comet) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->unbabel-comet) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->unbabel-comet) (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->unbabel-comet) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->unbabel-comet) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from cffi->fairseq==0.9.0->unbabel-comet) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from packaging->transformers<5.0.0,>=4.0.0->unbabel-comet) (2.4.7)\n",
      "Requirement already satisfied: portalocker==2.0.0 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from sacrebleu->fairseq==0.9.0->unbabel-comet) (2.0.0)\n",
      "Requirement already satisfied: pywin32!=226 in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from portalocker==2.0.0->sacrebleu->fairseq==0.9.0->unbabel-comet) (227)\n",
      "Requirement already satisfied: click in c:\\users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.0.0->unbabel-comet) (7.1.2)\n",
      "Collecting imageio>=2.3.0\n",
      "  Using cached imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "Collecting test-tube\n",
      "  Using cached test_tube-0.7.4.tar.gz (21 kB)\n",
      "  Using cached test_tube-0.7.3.tar.gz (21 kB)\n",
      "Collecting tb-nightly==1.15.0a20190708\n",
      "  Using cached tb_nightly-1.15.0a20190708-py3-none-any.whl (3.9 MB)\n",
      "Collecting test-tube\n",
      "  Using cached test_tube-0.7.2.tar.gz (21 kB)\n",
      "  Using cached test_tube-0.7.1.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.7.0.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.9.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.8.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.7.6.tar.gz (21 kB)\n",
      "  Using cached test_tube-0.6.7.5.tar.gz (21 kB)\n",
      "  Using cached test_tube-0.6.7.4.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.7.3.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.7.2.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.7.1.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.7.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.6.6.tar.gz (20 kB)\n",
      "  Using cached test_tube-0.2-py3-none-any.whl\n",
      "Building wheels for collected packages: fairseq, torch\n",
      "  Building wheel for fairseq (setup.py): started\n",
      "  Building wheel for fairseq (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for fairseq\n",
      "  Building wheel for torch (setup.py): started\n",
      "  Building wheel for torch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch\n",
      "Failed to build fairseq torch\n",
      "Installing collected packages: torch, tokenizers, test-tube, tensorboard-plugin-wit, scipy, sacremoses, sacrebleu, protobuf, markdown, huggingface-hub, grpcio, google-auth-oauthlib, cython, absl-py, transformers, tensorboard, sentencepiece, scikit-learn, pytorch-nlp, pytorch-lightning, fsspec, fairseq, unbabel-comet\n",
      "    Running setup.py install for torch: started\n",
      "    Running setup.py install for torch: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   command: 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\anton\\AppData\\Local\\Temp\\pip-wheel-3nj1q0ld'\n",
      "       cwd: C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\\\n",
      "  Complete output (30 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_deps\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\\setup.py\", line 225, in <module>\n",
      "      setup(name=\"torch\", version=\"0.1.2.post2\",\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages\\setuptools\\__init__.py\", line 153, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 299, in run\n",
      "      self.run_command('build')\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\command\\build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\\setup.py\", line 51, in run\n",
      "      from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n",
      "  ModuleNotFoundError: No module named 'tools.nnwrap'\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for torch\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' clean --all\n",
      "       cwd: C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\n",
      "  Complete output (2 lines):\n",
      "  running clean\n",
      "  error: [Errno 2] No such file or directory: '.gitignore'\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed cleaning build dir for torch\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\anton\\AppData\\Local\\Temp\\pip-record-o86ivv81\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\Include\\torch'\n",
      "         cwd: C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\\\n",
      "    Complete output (23 lines):\n",
      "    running install\n",
      "    running build_deps\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\\setup.py\", line 225, in <module>\n",
      "        setup(name=\"torch\", version=\"0.1.2.post2\",\n",
      "      File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\site-packages\\setuptools\\__init__.py\", line 153, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\\setup.py\", line 99, in run\n",
      "        self.run_command('build_deps')\n",
      "      File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"C:\\Users\\anton\\anaconda3\\envs\\textmining\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\anton\\AppData\\Local\\Temp\\pip-install-8s056mam\\torch_a3fa143634ef4758affaf0d22961c71a\\setup.py\", line 51, in run\n",
      "        from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n",
      "    ModuleNotFoundError: No module named 'tools.nnwrap'\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\anton\\\\AppData\\\\Local\\\\Temp\\\\pip-install-8s056mam\\\\torch_a3fa143634ef4758affaf0d22961c71a\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\anton\\AppData\\Local\\Temp\\pip-record-o86ivv81\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\anton\\anaconda3\\envs\\textmining\\Include\\torch' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06459d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11988be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataVectorization(train, dev, test):\n",
    "    \"\"\"\n",
    "    Function that receives train, development and test datasets and returns the sets vectorized.\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Create the CountVectorizer (Bag of words) object\n",
    "    baseline_encoder = CountVectorizer(max_features = 5000) # Settting limit for computational reasons\n",
    "    \n",
    "    train_vectorizer_reference = baseline_encoder.fit_transform(train['reference']).todense()\n",
    "    train_vectorizer_translation = baseline_encoder.transform(train['translation']).todense()\n",
    "    train_vectorizer = [train_vectorizer_reference, train_vectorizer_translation]\n",
    "    \n",
    "    for i, d in enumerate([dev, test]):\n",
    "        referece_vectorized = baseline_encoder.transform(d['reference']).todense()\n",
    "        translation_vectorized = baseline_encoder.transform(d['translation']).todense()\n",
    "        if i == 0:\n",
    "            dev_vectorizer = [referece_vectorized, translation_vectorized]\n",
    "        else:\n",
    "            test_vectorizer = [referece_vectorized, translation_vectorized]\n",
    "    \n",
    "    return train_vectorizer, dev_vectorizer, test_vectorizer\n",
    "\n",
    "def getBaseline(reference, translation, yLabels):\n",
    "    \"\"\"\n",
    "    Get baseline correlations for the given sets, computing the cosine similarity.\n",
    "    \n",
    "    \"\"\"   \n",
    "    cos = []\n",
    "    for i in range(reference.shape[0]):\n",
    "        cos.append(cosine_similarity(reference[i], translation[i])[0])\n",
    "    cos = np.array(cos)\n",
    "    cos.shape = (cos.shape[0],)\n",
    "    \n",
    "    return pearsonr(yLabels, cos), kendalltau(yLabels, cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbd892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list,\n",
    "          lower = False,\n",
    "          keep_numbers = False,\n",
    "          keep_expression = False,\n",
    "          remove_char = False,\n",
    "          remove_stop = False,\n",
    "          remove_tag = False,\n",
    "          lemmatize = False,\n",
    "          stemmer = False,\n",
    "          english = True\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    if english:\n",
    "        lang = 'english'\n",
    "    else:\n",
    "        lang = 'finnish'\n",
    "    \n",
    "    stop = set(stopwords.words(lang))\n",
    "    stem = SnowballStemmer(lang)\n",
    "    \n",
    "    updates = []\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "            \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if not keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        #KEEP '?' and '!' AS TOKENS\n",
    "        if not keep_expression:\n",
    "            text = re.sub(\"[\\?|\\!]\", 'EXPRESSION', text)\n",
    "            \n",
    "        #REMOVE TAGS\n",
    "        if remove_tag:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "            \n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        if remove_char:\n",
    "            text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            if english:\n",
    "                lemma = WordNetLemmatizer()\n",
    "                text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(stem.stem(word) for word in text.split())\n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates\n",
    "\n",
    "def clean_ch(text_list, keep_numbers=False, remove_punctuation=False, remove_stop = False, stopwords_set='merged'):\n",
    "    \"\"\"\n",
    "    Function that removes chinese stopwords\n",
    "    \n",
    "    :param stopwords_set: remove words of both sets (merged), just the 1st (fst) or just the second (snd) \n",
    "    \"\"\"\n",
    "    updates = []\n",
    "    \n",
    "    zh_stopwords1 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords1.txt', 'r', encoding='utf-8').readlines()]\n",
    "    zh_stopwords2 = [line.strip() for line in open('chinese_stopwords/chinese_stopwords2.txt', 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    if stopwords_set == 'merged':\n",
    "        stop = list(set(zh_stopwords1 + zh_stopwords2))\n",
    "    elif stopwords_set == 'fst':\n",
    "        stop = zh_stopwords1\n",
    "    elif stopwords_set == 'snd':\n",
    "        stop = zh_stopwords2\n",
    "\n",
    "    for j in range(len(text_list)):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        \n",
    "        #KEEP NUMBERS AS TOKENS\n",
    "        if keep_numbers:\n",
    "            text = re.sub(\"[\\d+]\", 'X', text)\n",
    "        \n",
    "        # REMOVE PUNCTUATION\n",
    "        if remove_punctuation:\n",
    "            # https://stackoverflow.com/questions/36640587/how-to-remove-chinese-punctuation-in-python\n",
    "            punc = \".\"\n",
    "            text = re.sub(r\"[%s]+\" %punc, \"\", text)\n",
    "        \n",
    "        # REMOVE STOP WORDS\n",
    "        if remove_stop:\n",
    "            text = ' '.join([word for word in text.split(' ') if word not in stop])\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce514b72",
   "metadata": {
    "id": "jA_9c3SpLTUn"
   },
   "source": [
    "# Data Importing and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb58d83f",
   "metadata": {
    "id": "nU_gOCWtF8XY"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('corpus')\n",
    "files.remove('.DS_Store')\n",
    "files.remove('scores_ru-en.csv')\n",
    "scaler = MinMaxScaler()\n",
    "for file_ in files:\n",
    "  name = file_.split('-')[0] + file_.split('-')[1]\n",
    "  vars()[name] = pd.read_csv(os.path.join('corpus', file_, 'scores.csv'))\n",
    "  vars()[name].drop(columns = ['source', 'annotators', 'z-score'], inplace = True)\n",
    "  vars()[name]['avg-score'] = scaler.fit_transform(vars()[name]['avg-score'].values.reshape(-1,1)) #normalizing values betwewen 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1ef1c2",
   "metadata": {
    "id": "EcUEx6elHvwd"
   },
   "outputs": [],
   "source": [
    "english = csen.copy()\n",
    "for df in [deen, ruen, zhen]:\n",
    "  english = english.append(df)\n",
    "english.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074be1cf",
   "metadata": {
    "id": "8ziEKCv4LFwE"
   },
   "outputs": [],
   "source": [
    "finnish = enfi.copy()\n",
    "chinese = enzh.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320b7c4",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cbf6d",
   "metadata": {},
   "source": [
    "Cleaning data accordinly to the config that maximized the correlation with the baseline. Results in baseline_AP.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77917775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = ['train', 'dev']\n",
    "cleaned_data = pd.DataFrame()\n",
    "cleaned_data['avg-score'] = finnish['avg-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    cleaned_data[column] = clean(finnish[column],\n",
    "                                lower = True,\n",
    "                                keep_numbers = False,\n",
    "                                keep_expression = False,\n",
    "                                remove_char = True,\n",
    "                                remove_stop = False,\n",
    "                                remove_tag = False,\n",
    "                                lemmatize = False,\n",
    "                                stemmer = True,\n",
    "                                english=False)\n",
    "\n",
    "fin_train, fin_dev = train_test_split(cleaned_data, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "fin_dev, fin_test = train_test_split(fin_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "fin_train_bl_encoded, fin_dev_bl_encoded, fin_test_bl_encoded = dataVectorization(fin_train, fin_dev, fin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b638cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = ['train', 'dev']\n",
    "cleaned_data = pd.DataFrame()\n",
    "cleaned_data['avg-score'] = chinese['avg-score']\n",
    "for column in ['reference', 'translation']:\n",
    "    cleaned_data[column] = clean_ch(chinese[column],\n",
    "                                keep_numbers = True,\n",
    "                                remove_punctuation = False,\n",
    "                                remove_stop = False,\n",
    "                                stopwords_set = 'merged')\n",
    "\n",
    "ch_train, ch_dev = train_test_split(cleaned_data, shuffle = True, test_size = 0.2, random_state = 7)\n",
    "ch_dev, ch_test = train_test_split(ch_dev, shuffle = True, test_size = 0.5, random_state = 7)\n",
    "\n",
    "ch_train_bl_encoded, ch_dev_bl_encoded, ch_test_bl_encoded = dataVectorization(ch_train, ch_dev, ch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f24ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8665c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
